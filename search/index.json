[{"content":"技术美术——高级纹理 立方体纹理(Cubemap) 立方体纹理（Cubemap） 是环境映射（Environment Mapping）的一种实现方法。环境映射可以模拟物体周围的环境，而使用了环境映射的物体可以看起来像镀了层金属一样 反射出周围的环境。\n优点\n 立方体纹理的实现简单快捷，而且得到的效果也比较好。  缺点\n 当场景中引入了新的物体、光源，或者物体发生移动时，我们就需要重新生成立方体纹理。 立方体纹理也仅可以反射环境，但不能反射使用了该立方体纹理的物体本身。这是因为，立方体纹理不能模拟多次反射的结果。  综合考虑下来，立方体纹理适用于凸面体，而不太适用于凹面体 ( 因为凹面体会反射自身 )。\n立方体纹理—用于环境映射 立方体纹理最常见的用处是用于环境映射。通过这种方法，我们可以 模拟出金属质感的材质。\n创建用于环境映射的立方体纹理的三种方法：\n 直接由一些特殊布局的纹理创建； 手动创建一个Cubemap资源，再把6张图赋给它； 由脚本生成。  第一种方法：提供一张具有特殊布局的纹理，例如类似立方体展开图的交叉布局、全景布局等（就类似于人物贴图，换成了球形）。然后只需把该纹理的 Texture Type 设置为 Cubemap 即可，Unity 会做好剩下的事情。\n第二种方法：先在项目资源中创建一个 Cubemap，然后把它的 6 张纹理拖拽到它的面板中。在 Unity 5 中，官方推荐使用第一种方法创建立方体纹理，这是因为第一种方法可以对纹理数据进行压缩，即可以支持边缘修正、光滑反射（glossy reflection）和 HDR 等功能。\n第三种方法：使用 Camera.RenderToCubemap 函数来实现，Camera.RenderToCubemap 函数可以把任意位置观察到的场景图像存储到 6 张图像中，从而创建出该位置上对应的立方体纹理。\n  创建一个编辑器脚本，用于将摄像机照射到的图片渲染到 Cubemap 中。由于该代码需要添加菜单条目，因此我们需要把它放在 Editor 文件夹下才能正确执行。原理如下:\n在 renderFromPosition（由用户指定）位置处动态创建一个摄像机，并调用 Camera.RenderToCubemap 函数把从当前位置观察到的图像渲染到用户指定的立方体纹理 cubemap 中，完成后再销毁临时摄像机。\nusing UnityEngine; using UnityEditor; using System.Collections; public class RenderCubemapWizard : ScriptableWizard { public Transform renderFromPosition; public Cubemap cubemap; void OnWizardUpdate () { helpString = \u0026#34;选择要渲染的坐标位置和要渲染的cubemap\u0026#34;; isValid = (renderFromPosition != null) \u0026amp;\u0026amp; (cubemap != null); } void OnWizardCreate () { // 创建用于渲染的临时摄像机  GameObject go = new GameObject( \u0026#34;CubemapCamera\u0026#34;); go.AddComponent\u0026lt;Camera\u0026gt;(); // 把它放到物体坐标上  go.transform.position = renderFromPosition.position; // 渲染成cubemap  go.GetComponent\u0026lt;Camera\u0026gt;().RenderToCubemap(cubemap); // 销毁临时相机  DestroyImmediate( go ); } [MenuItem(\u0026#34;GameObject/Render into Cubemap\u0026#34;)] static void RenderCubemap () { ScriptableWizard.DisplayWizard\u0026lt;RenderCubemapWizard\u0026gt;( \u0026#34;Render cubemap\u0026#34;, \u0026#34;Render!\u0026#34;); } }   新建一个用于存储的立方体纹理（在 Project 视图下单击右键，选择 Create -\u0026gt; Legacy -\u0026gt; Cubemap来创建）。为了让脚本可以顺利将图像渲染到该立方体纹理中，我们需要在它的面板中勾选 Readable 选项。\n  从 Unity 菜单栏选择 GameObject -\u0026gt; Render into Cubemap，打开我们在脚本中实现的用于渲染立方体纹理的窗口，并把第一步创建的 GameObject 和第二步中的纹理分别拖拽到窗口中的 Render From Position 和 Cubemap 选项。\n  单击窗口的 Render！按钮，就可以把从该位置观察到的世界空间下的 6 张图像中渲染到纹理中。\n  需要注意的是，我们需要为 Cubemap 设置大小，即上图中的 Face size 选项。Face size 值越大，渲染出来的立方体纹理分辨率越大，效果可能更好，单需要占用的内存也越大，这可以由面板最下方显示的内存大小得到。\n反射 模拟反射效果很简单，我们只需要通过 入射光线的方向 和 表面法线方向来计算反射方向，再利用 反射方向对立方体纹理采样 即可。\n反射用到 Shader 的是在漫反射模型上进行修改的：\n 增添三个属性，分别用于控制反射颜色、反射程度和捕捉环境映射纹理 在顶点着色器中哪个使用reflect函数来计算该顶点的反射方向   reflect ( I, N ) 根据入射光线方向I和表面法向量N计算反射向量，仅对三元向量有效\n 在片元着色器中使用 texCUBE 函数和顶点着色器中获得的反射方向对立方体纹理进行采样，得到反射颜色 最后使用 lerp 对原有漫反射与纹理反射颜色进行插值  Shader \u0026#34;Unlit/Reflect\u0026#34; { Properties { _Color(\u0026#34;Color\u0026#34;, Color) = (1,1,1,1) _ReflectColor(\u0026#34;Reflect Color\u0026#34;, Color) = (1,1,1,1) _ReflectPower(\u0026#34;Reflect Power\u0026#34;, Range(0.0, 1)) = 0.2 _CubeMap(\u0026#34;CubeMap\u0026#34;, Cube) = \u0026#34;_Skybox\u0026#34; {} } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } Pass { Tags {\u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34;} CGPROGRAM #pragma multi_compile_fwdbase #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; float4 _Color; float4 _ReflectColor; float _ReflectPower; samplerCUBE _CubeMap; struct appdata { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; float3 worldPos : TEXCOORD1; float3 worldRefl : TEXCOOR2; SHADOW_COORDS(3) }; v2f vert (appdata v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; o.worldRefl = reflect(-UnityWorldSpaceViewDir(o.worldPos), o.worldNormal); // 反射方向 TRANSFER_SHADOW(o); return o; } fixed4 frag (v2f i) : SV_Target { half3 worldNormal = normalize(i.worldNormal); half3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); half3 halfDir = normalize(worldLightDir + worldViewDir); half NdotL = max(0.0, dot(worldNormal, worldLightDir)); half3 reflection = texCUBE(_CubeMap, i.worldRefl).rgb * _ReflectColor.rgb; // 采样 Cubemap 计算反射 half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; half3 diffuse = _LightColor0.rgb * _Color.rgb * NdotL; UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); return half4(ambient + lerp(diffuse, reflection, _ReflectPower) * atten, 1.0); } ENDCG } } Fallback \u0026#34;Diffuse\u0026#34; } 折射 给定入射角时，我们可以使用 斯涅尔定律 (Snell’s law ) 来计算反射角。当光从介质 1 沿着和表面法线夹角为 θ1 的方向斜射入介质 2 时，我们可以使用如下公式计算折射光线与法线的夹角θ2：\n η1 sinθ1 = η2 sinθ2\n 其中 η1 和 η2 分别是两个介质的 折射率 (index of refraction ) 。折射率是一项重要的物理常量，例如真空的折射率是 1，而玻璃的折射率一般是 1.5。\n折射用到 Shader 中与反射类似，依旧使用 refract 函数来计算该顶点的折射方向，但不同的是采用了三个参数：\n 第一个参数即为入射光线方向，它必须是归一化后的矢量； 第二个参数是表面法线，法线方向同样是要归一化后的； 第三个参数是入射光线所在介质的折射率和折射光线所在介质的折射率之间的比值。  Shader \u0026#34;Unlit/Refract\u0026#34; { Properties { _Color(\u0026#34;Color\u0026#34;, Color) = (1,1,1,1) _RefractColor(\u0026#34;Refract Color\u0026#34;, Color) = (1,1,1,1) _RefractPower(\u0026#34;Refract Power\u0026#34;, Range(0.0, 1)) = 1 _RefractRatio(\u0026#34;Refract Ratio\u0026#34;, Range(0.01, 1)) = 0.2 _Cubemap(\u0026#34;CubeMap\u0026#34;, Cube) = \u0026#34;_Skybox\u0026#34; { } } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } Pass { Tags {\u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34;} CGPROGRAM #pragma multi_compile_fwdbase #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; float4 _Color; float4 _RefractColor; float _RefractPower; float _RefractRatio; samplerCUBE _Cubemap; struct appdata { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 worldPos : TEXCOORD0; float3 worldNormal : TEXCOORD1; float3 worldRefra : TEXCOORD2; SHADOW_COORDS(3) }; v2f vert (appdata v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; o.worldRefra = refract(-normalize(UnityWorldSpaceViewDir(o.worldPos)), normalize(o.worldNormal), _RefractRatio); // 计算折射方向 TRANSFER_SHADOW(o); return o; } fixed4 frag (v2f i) : SV_Target { half3 worldNormal = normalize(i.worldNormal); half3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); half NdotL = max(0.0, dot(worldNormal, worldLightDir)); half3 refraction = texCUBE(_Cubemap, i.worldRefra).rgb * _RefractColor.rgb; // 采样 Cubemap 计算折射 half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; half3 diffuse = _LightColor0.rgb * _Color.rgb * NdotL; UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); return half4(ambient + lerp(diffuse, refraction, _RefractPower) * atten, 1.0); } ENDCG } } Fallback \u0026#34;Reflective/VertexLit\u0026#34; } 菲涅尔反射 使用 菲涅尔反射 (Fresnel reflection ) 来根据视角方向控制反射过程，菲涅尔反射描述了一种光学现象，即当光线照射到物体表面上时，一部分发生反射，一部分进入物体内部，发生折射或散射。被反射的光和入射光之间存在一定的比率关系，这个比率关系可以通过菲涅尔等式进行计算。\n一个经常使用的例子是，当你站在湖边，直接低头看脚边的水面时，你会发现水几乎是透明的，你可以直接看到水底的小鱼和石子；但是当你抬头看远处的水面时，会发现几乎看不到水下的情景，而只能看到水面反射的环境。这就是所谓的菲涅尔效果。\n真实世界的菲涅尔等式是非常复杂的，但在实时渲染中，我们通常会使用一些近似公式来计算。其中一个著名的计算公式就是 Schlick菲涅尔 近似等式：\n FSchlick ( v , n ) = F0 + ( 1 - F0 ) ( 1 - v · n)5\n 其中，F0 是一个反射系数，用于控制菲涅尔反射的强度，v 是视角方向，n 是表面法线。另一个应用比较广泛的等式是 Empricial菲涅尔 近似等式：\n FEmpricial ( v , n ) = max ( 0 , min ( 1 , bias + scale * ( 1 - v · n ) power))\n 其中 bias、scale 和 power 是控制项。\n下面将使用 Schlick菲涅尔 近似等式来模拟菲涅尔反射。在片元着色器中，我们套用 Schlick 菲涅尔公式来计算菲尼尔比率，然后用该比率插值混合 漫反射光照 和 反射光照 。\nShader \u0026#34;Unlit/Fresnel\u0026#34; { Properties { _Color(\u0026#34;Color\u0026#34;, Color) = (1,1,1,1) _FresnelScale(\u0026#34;Fresnel Scale\u0026#34;, Range(0.0, 1)) = 0.5 _CubeMap(\u0026#34;Cube Map\u0026#34;, Cube) = \u0026#34;_Skybox\u0026#34; } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } Pass { Tags {\u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34;} CGPROGRAM #pragma multi_compile_fwdbase #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; float4 _Color; float _FresnelScale; samplerCUBE _CubeMap; struct appdata { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; float3 worldPos : TEXCOORD1; float3 worldRefl : TEXCOORD2; SHADOW_COORDS(3) }; v2f vert (appdata v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.vertex); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; o.worldRefl = reflect(-UnityWorldSpaceViewDir(o.worldPos), o.worldNormal); // 计算反射方向 TRANSFER_SHADOW(o); return o; } fixed4 frag (v2f i) : SV_Target { half3 worldNormal = normalize(i.worldNormal); half3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); half NdotL = max(0.0, dot(worldNormal, worldLightDir)); half3 reflection = texCUBE(_CubeMap, i.worldRefl).rgb; // 采样 Cubemap half fresnel = _FresnelScale + (1.0 - _FresnelScale) * pow((1.0 - dot(worldViewDir, worldNormal)), 5.0); // 计算 Fresnel 效果 half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; half3 diffuse = _LightColor0.rgb * _Color.rgb * NdotL; UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); return half4(ambient + lerp(diffuse, reflection, saturate(fresnel)) * atten, 1.0); } ENDCG } } \u0026#34;Reflective/VertexLit\u0026#34; } 渲染纹理 一个摄像机的渲染结果会输出到颜色缓冲中，并显示到我们的屏幕上。现代的 GPU 允许我们把整个三维场景渲染到一个中间缓存中，即 渲染目标纹理 (Render Target Texture，RTT) ，而不是传统的 帧缓冲或后备缓冲 ( back buffer ) 。与之相关的是 多重渲染目标 ( Multiple Render Target ，MRT ) ，这种技术指的是 GPU 允许我们把场景同时渲染到多个目标纹理中，而不再需要为每个渲染目标纹理单独渲染完整的场景。延迟渲染就是使用多重渲染目标的一个应用。\nUnity 为渲染目标纹理定义了一种专门的纹理类型—— 渲染纹理 ( Render Texture ) 。在 Unity 中使用渲染纹理通常有两种方式：\n 一种方式是在 Project 目录下创建一个 渲染纹理 ，然后把某个摄像机的 渲染目标 设置成该 渲染纹理 ，这样一来该摄像机的渲染结果就会实时更新到 渲染纹理 中，而不会显示在屏幕上。使用这种方法，我们还可以选择纹理的分辨率、滤波模式等纹理属性。 另一种方式是在屏幕后处理时使用 GrabPass 命令或 OnRenderImage 函数来获取当前屏幕图像，Unity 会把这个屏幕图像放到一张和屏幕分辨率等同的渲染纹理中，下面我们可以在自定义的 Pass 中把它们当成普通纹理来处理，从而实现各种屏幕特效。  镜子效果 关键在于新建一个摄像机，一个 Render Texture，将新建的 Render Texture 当作新建相机的 Target Texture ，新建一个 Shader，shader 里声明一个 2D 纹理属性，将新建的 Render Texture 赋值给它。\n 创建一个四边形（Quad），调整它的位置和大小，用于充当镜子。 为了得到从镜子出发观察到的场景图像，我们需要创建一个摄像机，并调整它的位置、裁剪平面、视角等，使得它显示的图像是我们希望的镜子的图像。 由于这个摄像机不需要直接显示在屏幕上，而是用于渲染纹理。因此我们可以直接创建一个 Texture 并拖拽到该摄像机的 Target Texture 上。  关于 Quad 的着色器实现也非常简单，只需要声明一个纹理属性，然后将该纹理进行左右翻转后输出。\nShader \u0026#34;Unlit/Mirror\u0026#34; { Properties { _MainTex (\u0026#34;Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } LOD 100 Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; struct appdata { float4 vertex : POSITION; float2 texcoord : TEXCOORD0; }; struct v2f { float4 pos : SV_POSITION; float2 uv : TEXCOORD0; }; v2f vert (appdata v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv = v.texcoord; o.uv.x = 1 - o.uv.x; // 反转 x 轴 return o; } fixed4 frag (v2f i) : SV_Target { fixed4 col = tex2D(_MainTex, i.uv); return col; } ENDCG } } FallBack Off } 玻璃效果 Unity 中，还可以在 Unity Shader 中使用一种特殊的 Pass 来完成获取屏幕图像的目的，这就是 GrabPass 。通常会使用 GrabPass 来实现诸如 玻璃等透明材质的模拟 ，与使用简单的透明混合不同，使用 GrabPass 可以让我们对物体后面的图像进行更复杂的处理，例如使用法线来模拟折射效果，而不再是简单的和原屏幕颜色混合。\n当我们在 Shader 中定义了一个 GrabPass 后，Unity 会把当前屏幕的图像绘制在一张纹理中，以便我们在后续的 Pass 中访问它。需要注意的是，在使用 GrabPass 的时候，我们需要额外小心物体的渲染队列设置。正如之前所说，GrabPass 通常用于渲染透明物体，尽管代码里并不包含混合指令，但我们往往仍然需要把物体的 渲染队列 设置成 透明队列 （即 “Queue”=“Transparent” ）。这样才能保证渲染物体时，所有的不透明物体都已经被绘制在屏幕上，从而获得正确的屏幕图像。\nGrabPass 的两种形式  直接使用 GrabPass{} ，然后在后续的 Pass 中直接使用 _GrabTexture 来访问屏幕图像。但是当场景中有多个物体都使用了这样的形式来抓取屏幕时，这种方法的性能消耗比较大，因为对于每一个使用它的物体，Unity 都会为它单独进行一次昂贵的屏幕抓取操作。但这种方法可以让每个物体得到不同的屏幕图像，这取决于它们的渲染队列及渲染它们时当前的屏幕缓冲中的颜色。 使用 GrabPass{“TextureName”} ，正如本节所实现，我们可以在后续的 Pass 中使用 TextureName 来访问屏幕图像。使用这种方法同样可以抓取屏幕，但 Unity 只会在每一帧为第一个使用名为 TextureName 的纹理的物体执行一次屏幕抓取操作，而这个纹理同时也可以在其它 Pass 中被访问。这种方法更加高效，因为不管场景中有多少物体使用了该命令，每一帧中 Unity 都只会执行一次抓取操作，这也意味着所有物体都会使用同一张屏幕图像。不过，在大多数情况下这已经足够了。  在本节中，我们将会使用 GrabPass 来模拟一个玻璃效果。\n思路 这种效果实现非常简单，我们首先使用一张法线纹理来修改模型的法线信息，然后使用反射的方法，通过一个 Cubemap 来模拟玻璃反射，而在模拟折射时，则使用了 GrabPass 获取玻璃后面的屏幕图像，并使用切线空间下的法线对屏幕纹理坐标偏移后，再对屏幕图像进行采样来模拟近似的折射效果。\n具体实现  搭建环境：建立一个房间，然后放置了一个立方体和球体，其中球体位于立方体内部，这是为了模拟玻璃对内部物体的折射效果。创建一个着色器付给立方体。 在着色器中，我们把 Queue 设置成 Transparent 可以保证该物体渲染时，其它所有不透明物体都已经被渲染到屏幕上了，否则就可能无法正确得到“透过玻璃看到的图像”。 然后设置 RenderType 为 Opaque 则是为了在使用 着色器替换 ( Shader Replacement ) 时，该物体可以在需要时被正确渲染。这通常发生在我们需要得到摄像机的深度和法线纹理时。 通过关键词 CrabPass 定义了一个抓取屏幕图像的 Pass。在这个 Pass 中我们定义了一个字符串，该字符串内部的名称决定了抓取到的屏幕图像将会被存入哪个纹理中。实际上，我们可以省略声明该字符串，但直接声明纹理名称的方法往往可以得到更高的性能。 定义了 _RefractionTex 和 _RefractionTex_TexelSize 变量，这对应了在使用 GrabPass 时指定的纹理名称。 _RefractionTex_TexelSize 可以让我们得到该像素的纹理大小，例如一个大小为 256×512 的纹理，它的像素大小为（1/256, 1/512）。我们需要在对屏幕图像的采样坐标进行偏移时使用该量。 在顶点着色器中通过调用内置的 ComputeGrabScreenPos 函数来得到对应被抓取的屏幕图像的采样坐标。 在片元着色器中我们对法线纹理进行采样，得到切线空间下的法线方向。我们使用该值和 _Distortion 属性以及 _RefractionTex_TexelSize 来对屏幕图像的采样所需的坐标进行偏移，模拟折射效果。 _Distortion 值越大，偏移量越大，玻璃背后的物体看起来变形程度越大。 在对屏幕纹理进行采样时使用了 齐次除法 获取视口坐标下的坐标：( i.scrPos.xy / i.scrPos.w ) 最后，我们使用 _RefractAmount 属性对反射和折射颜色进行混合，作为最终的输出颜色。  Shader \u0026#34;Unlit/Glass\u0026#34; { Properties { _MainTex (\u0026#34;MainTex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _BumpTex(\u0026#34;BumpTex\u0026#34;, 2D) = \u0026#34;bump\u0026#34; {} _Cubemap(\u0026#34;Cube Map\u0026#34;, Cube) = \u0026#34;_Skybox\u0026#34; {} _Distortion(\u0026#34;Distortion\u0026#34;, Range(0.0, 100)) = 10 _RefractAmount(\u0026#34;RefractAmount\u0026#34;, Range(0.0, 1.0)) = 1.0 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34; } GrabPass { \u0026#34;_RefractionTex\u0026#34; // GrabPass：用于抓取屏幕图像的 pass，内部字符串决定输出纹理名称 } Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; sampler2D _BumpTex; float4 _BumpTex_ST; samplerCUBE _Cubemap; float _Distortion; float _RefractAmount; sampler2D _RefractionTex; // 获取屏幕图像输出的纹理 float4 _RefractionTex_TexelSize; // 获取纹理的纹素大小 struct appdata { float4 vertex : POSITION; float4 texcoord : TEXCOORD0; float3 normal : NORMAL; float4 tangent : TANGENT; }; struct v2f { float4 pos : SV_POSITION; float4 uv : TEXCOORD0; float4 screenPos : TEXCOORD1; // 屏幕采样坐标 float4 TtoW0 : TEXCOORD2; float4 TtoW1 : TEXCOORD3; float4 TtoW2 : TEXCOORD4; }; v2f vert (appdata v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv.xy = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; // 缩放和平移 o.uv.zw = v.texcoord.xy * _BumpTex_ST.xy + _BumpTex_ST.zw; // 缩放和平移 o.screenPos = ComputeGrabScreenPos(o.pos); // ComputeGrabScreenPos 获取抓取屏幕的采样坐标 float3 worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; float3 worldNormal = UnityObjectToWorldNormal(v.normal); float3 worldTangent = UnityObjectToWorldDir(v.tangent.xyz); float3 worldBinormal = cross(worldNormal, worldTangent) * v.tangent.w; o.TtoW0 = float4(worldTangent.x, worldBinormal.x, worldNormal.x, worldPos.x); o.TtoW1 = float4(worldTangent.y, worldBinormal.y, worldNormal.y, worldPos.y); o.TtoW2 = float4(worldTangent.z, worldBinormal.z, worldNormal.z, worldPos.z); return o; } fixed4 frag (v2f i) : SV_Target { half3 worldPos = float3(i.TtoW0.w, i.TtoW1.w, i.TtoW2.w); half3 worldLightDir = normalize(UnityWorldSpaceLightDir(worldPos)); half3 worldViewDir = normalize(UnityWorldSpaceViewDir(worldPos)); half3 bump = UnpackNormal(tex2D(_BumpTex, i.uv.zw)); half2 offset = bump.xy * _Distortion * _RefractionTex_TexelSize.xy; i.screenPos.xy = offset * i.screenPos.z + i.screenPos.xy; half3 refrCol = tex2D(_RefractionTex, i.screenPos.xy / i.screenPos.w).rgb; bump = normalize(half3(dot(i.TtoW0.xyz, bump), dot(i.TtoW1.xyz, bump), dot(i.TtoW2.xyz, bump))); half3 reflDir = reflect(-worldViewDir, bump); half4 texColor = tex2D(_MainTex, i.uv.xy); half3 reflCol = texCUBE(_Cubemap, reflDir).rgb * texColor.rgb; half3 col = reflCol * (1 - _RefractAmount) + refrCol * _RefractAmount; return half4(col, 1.0); } ENDCG } } Fallback \u0026#34;Diffuse\u0026#34; } 渲染纹理 VS GrabPass  GrabPass 的好处在于实现简单，我们只需在 Shader 中写几行代码就可以实现抓取屏幕的问题。 从效率上来讲，使用渲染纹理的效率往往要好于 GrabPass ，尤其在移动设备上。使用渲染纹理我们可以自定义渲染纹理的大小，尽管这种方法需要把部分场景再次渲染一遍，但我们可以通过调整摄像机的渲染层来减少二次渲染时的场景大小，或使用其它方法来控制摄像机是否需要开启。  GrabPass 获取到的图像分辨率和显示屏幕是一致的，这意味着在一些高分辨率的设备上可能会造成严重的带宽影响。\n在移动设备上，GrabPass 虽然不会重新渲染场景，但它往往需要 CPU 直接读取后备缓冲（back buffer）中的数据，破坏了 CPU 和 GPU 之间的并行性，这是比较耗时的，甚至在一些移动设备上这是不支持的。\n命令缓存 (Command Buffers) 在 Unity5 中，Unity 引入了 命令缓冲（Command Buffers） 来允许我们扩展 Unity 的渲染流水线。使用命令缓冲我们也可以得到类似抓屏的效果，它可以在不透明物体渲染后把当前的图像复制到一个临时的渲染目标纹理中，然后在那里进行一些额外的操作，例如模糊等，最后把图像传递给需要使用它的物体进行处理和显示。\n有关命令缓冲区更多知识，移步：图形命令缓冲区 - Unity 手册\n程序纹理 程序纹理（Procedural Texture） 指的是那些由计算机生成的图像，我们通常使用一些特定的算法来创建个性化图案或非常真实的自然元素，例如木头、石子等。\n使用 程序纹理 的好处在于我们可以使用各种参数来控制纹理的外观，而这些属性不仅仅是那些颜色属性，甚至可以是完全不同类型的图案属性，这是我们可以得到更加丰富的动画和视觉效果。\nusing UnityEngine; using System.Collections; using System.Collections.Generic; [ExecuteInEditMode]//用于编辑器下运行 public class ProceduralTextureGeneration : MonoBehaviour { // 声明材质  public Material material = null; #region Material properties//材质属性  // 纹理大小 [SerializeField, SetProperty(\u0026#34;textureWidth\u0026#34;)] // 值通常是2的整数幂  private int m_textureWidth = 512; public int textureWidth { get { return m_textureWidth; } set { m_textureWidth = value; _UpdateMaterial(); } } // 纹理背景颜色 [SerializeField, SetProperty(\u0026#34;backgroundColor\u0026#34;)] private Color m_backgroundColor = Color.white; public Color backgroundColor { get { return m_backgroundColor; } set { m_backgroundColor = value; _UpdateMaterial(); } } // 圆点颜色 [SerializeField, SetProperty(\u0026#34;circleColor\u0026#34;)] private Color m_circleColor = Color.yellow; public Color circleColor { get { return m_circleColor; } set { m_circleColor = value; _UpdateMaterial(); } } // 模糊因子 [SerializeField, SetProperty(\u0026#34;blurFactor\u0026#34;)] // 用来磨合圆形边界  private float m_blurFactor = 2.0f; public float blurFactor { get { return m_blurFactor; } set { m_blurFactor = value; _UpdateMaterial(); } } #endregion  /// \u0026lt;summary\u0026gt;生成的纹理\u0026lt;/summary\u0026gt;  private Texture2D m_generatedTexture = null; // 初始化  void Start() { // 检测材质是否为空  if (material == null) { // 获取渲染器  Renderer renderer = gameObject.GetComponent\u0026lt;Renderer\u0026gt;(); if (renderer == null) { Debug.LogWarning(\u0026#34;找不到渲染器。\u0026#34;); return; } // 获取渲染器的材质  material = renderer.sharedMaterial; } _UpdateMaterial(); } /// \u0026lt;summary\u0026gt;  /// 生成纹理  /// \u0026lt;/summary\u0026gt;  private void _UpdateMaterial() { // 材质不为空  if (material != null) { // 调用方法生成程序纹理  m_generatedTexture = _GenerateProceduralTexture(); // 将纹理赋值给材质  material.SetTexture(\u0026#34;_MainTex\u0026#34;, m_generatedTexture); } } private Color _MixColor(Color color0, Color color1, float mixFactor) { Color mixColor = Color.white; mixColor.r = Mathf.Lerp(color0.r, color1.r, mixFactor); mixColor.g = Mathf.Lerp(color0.g, color1.g, mixFactor); mixColor.b = Mathf.Lerp(color0.b, color1.b, mixFactor); mixColor.a = Mathf.Lerp(color0.a, color1.a, mixFactor); return mixColor; } /// \u0026lt;summary\u0026gt;  /// 生成程序纹理  /// \u0026lt;/summary\u0026gt;  /// \u0026lt;returns\u0026gt;\u0026lt;/returns\u0026gt;  private Texture2D _GenerateProceduralTexture() { Texture2D proceduralTexture = new Texture2D(textureWidth, textureWidth); // 定义圆与圆之间的间距  float circleInterval = textureWidth / 4.0f; // 定义圆的半径  float radius = textureWidth / 10.0f; // 定义模糊系数  float edgeBlur = 1.0f / blurFactor; for (int w = 0; w \u0026lt; textureWidth; w++) { for (int h = 0; h \u0026lt; textureWidth; h++) { // 使用背景颜色进行初始化  Color pixel = backgroundColor; // 依次画九个圆  for (int i = 0; i \u0026lt; 3; i++) { for (int j = 0; j \u0026lt; 3; j++) { // 定义圆的圆心  Vector2 circleCenter = new Vector2(circleInterval * (i + 1), circleInterval * (j + 1)); // 计算当前像素与圆心的距离  float dist = Vector2.Distance(new Vector2(w, h), circleCenter) - radius; // 模糊圆的边界  Color color = _MixColor(circleColor, new Color(pixel.r, pixel.g, pixel.b, 0.0f), Mathf.SmoothStep(0f, 1.0f, dist * edgeBlur)); // 与之前得到的颜色进行混合  pixel = _MixColor(pixel, color, color.a); } } // 写入像素  proceduralTexture.SetPixel(w, h, pixel); } } // 将像素值写入纹理  proceduralTexture.Apply(); return proceduralTexture; } } 总结 常规反射\nv2f vert (appdata v) { ... o.worldRefl = reflect(-UnityWorldSpaceViewDir(o.worldPos), o.worldNormal); // 反射方向 return o; } fixed4 frag (v2f i) : SV_Target { ... half3 reflection = texCUBE(_CubeMap, i.worldRefl).rgb * _ReflectColor.rgb; half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; half3 diffuse = _LightColor0.rgb * _Color.rgb * NdotL; return half4(ambient + lerp(diffuse, reflection, _ReflectPower), 1.0); } 常规折射\nv2f vert (appdata v) { ... o.worldRefra = refract(-normalize(UnityWorldSpaceViewDir(o.worldPos)), normalize(o.worldNormal), _RefractRatio); return o; } fixed4 frag (v2f i) : SV_Target { ... half3 refraction = texCUBE(_Cubemap, i.worldRefra).rgb * _RefractColor.rgb; half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; half3 diffuse = _LightColor0.rgb * _Color.rgb * NdotL; return half4(ambient + lerp(diffuse, refraction, _RefractPower), 1.0); } 菲涅尔反射\nv2f vert (appdata v) { ... o.worldRefl = reflect(-UnityWorldSpaceViewDir(o.worldPos), o.worldNormal); return o; } fixed4 frag (v2f i) : SV_Target { ... half3 reflection = texCUBE(_CubeMap, i.worldRefl).rgb; half fresnel = _FresnelScale + (1.0 - _FresnelScale) * pow((1.0 - dot(worldViewDir, worldNormal)), 5.0); half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; half3 diffuse = _LightColor0.rgb * _Color.rgb * NdotL; return half4(ambient + lerp(diffuse, reflection, saturate(fresnel)), 1.0); } 镜子\nv2f vert(a2v v) { ... o.uv = v.texcoord; // 需要镜像图像 o.uv.x = 1 - o.uv.x; return o; } fixed4 frag(v2f i): SV_Target { // 对纹理采样输出 return tex2D(_MainTex, i.uv); } 玻璃\nv2f vert (appdata v) { ... o.screenPos = ComputeGrabScreenPos(o.pos); // ComputeGrabScreenPos 获取抓取屏幕的采样坐标 ... return o; } fixed4 frag (v2f i) : SV_Target { ... half2 offset = bump.xy * _Distortion * _RefractionTex_TexelSize.xy; i.screenPos.xy = offset * i.screenPos.z + i.screenPos.xy; half3 refrCol = tex2D(_RefractionTex, i.screenPos.xy / i.screenPos.w).rgb; bump = normalize(half3(dot(i.TtoW0.xyz, bump), dot(i.TtoW1.xyz, bump), dot(i.TtoW2.xyz, bump))); half3 reflDir = reflect(-worldViewDir, bump); half4 texColor = tex2D(_MainTex, i.uv.xy); half3 reflCol = texCUBE(_Cubemap, reflDir).rgb * texColor.rgb; half3 col = reflCol * (1 - _RefractAmount) + refrCol * _RefractAmount; return half4(col, 1.0); } ","date":"2021-06-04T20:00:31+08:00","image":"https://yantree.github.io/p/shader%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81-%E9%AB%98%E7%BA%A7%E7%BA%B9%E7%90%86/topimg_hufdc4047d1f13243dc00ee98ca40c3721_1787439_120x120_fill_box_smart1_2.png","permalink":"https://yantree.github.io/p/shader%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81-%E9%AB%98%E7%BA%A7%E7%BA%B9%E7%90%86/","title":"Shader入门精要-高级纹理"},{"content":"技术美术——更加复杂的光照 前言 前面的学习里，场景中只有一个光源且为平行光，但在实际的项目中，我们需要处理数目更多、类型更加复杂的光源，最重要的是需要得到 阴影。这里，我们有必要知道 Unity 底层渲染引擎如何让我们在 shader 中访问它们的。\nUnity 渲染路径 设置项目的渲染路径 Unity 主要有三种渲染路径：前向渲染路径、延迟渲染路径、顶点照明渲染路径，其中顶点照明渲染路径已经被淘汰，其次新的延迟渲染路径代替了原来的延迟渲染路径。\n在项目中，整个项目会设置为其中一个渲染路径，默认情况下是 前向渲染路径，如果希望使用多个渲染路径，我们可以在摄像机的渲染路径设置中设置该摄像机使用的渲染路径，覆盖 Graphics 中的设置。\n设置 Pass 的渲染路径 完成以上设置，我们就可以在每个 Pass 中使用标签来指定该 Pass 使用的渲染路径。这是通过设置 Pass 的 LightMode 标签实现的。例如：\nPass { Tags { \u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34; } } 上面代码告诉 Unity 该 Pass 使用前向渲染路径中的 ForwardBase 路径。Pass 的 LightMode 标签支持的渲染路径设置选项。\n   标签名 描述     Always 不管使用哪种渲染路径，该 Pass 总会被渲染，但 不会计算光照   ForwardBase 仅用于 前向渲染，该 Pass 会计算环境光、最重要的平行光、逐顶点/ SH光源和 Lightmaps   ForwardAdd 仅用于 前向渲染，该 Pass 会计算额外的逐像素光源，每个 Pass 对应一个光源   Deferred 仅用于 延迟渲染，该 Pass 会渲染 G 缓冲（G-buffer）   ShadowCaster 把物体的深度信息渲染到阴影映射纹理（shadowmap）或一张深度纹理中    指定渲染路径的作用 借用网上一个例子，不同的渲染路径差距，如同不同的绘画方式\n \n指定渲染路径就是告诉 Unity 底层渲染引擎以什么渲染流程去准备光照属性与光源信息。如果没有指定任何渲染路径，那么一些光照变量很可能不会被正确赋值，计算出的效果很可能就是错误的。\n前向渲染路径 前向渲染路径的原理 前向渲染的光照计算由一个 Pass 块来完成，我们在这个 Pass 块中计算平行光或者其他光源。它的缺点是每个光源都要被 Pass 计算一遍才行。一个物体 n 个光源，计算 1 * n 次，m 个物体 n 个光源，计算 m * n 次。在实际情况中，后一个光源可能把前一个光源给覆盖了(由于光源强度等问题)。\n缺点：很多时候如果场景中光源过多，那么前向渲染会做非常多根本不需要的工作，前向渲染路径里面不能使用过多的光源。否则性能就会急速下降。\n为了改善前向渲染的问题，引擎通常会限制每个物体的逐像素光照数目。前向渲染的伪代码如下：\nPass{ for(each primitive in this model){ for(each fragment covered by this primitive){ if(failed in depth test){ //若片元没通过深度测试，舍弃 }else{ //若片元可见，进行光照的计算 float4 color = Shading(materialInfo,pos,normal,lightDir,viewDir); //写入帧缓存 writeFrameBuffer(fragment,color); } } } } Unity 中的前向渲染 Unity 中，前向渲染有 3 种处理光照 (照亮物体) 的方式：逐顶点处理、逐像素处理、球谐函数处理 ( Spherical Harmonics，SH )。\n哪种方式处理光照 决定一个光源按哪种方式处理光照取决于自身的类型和渲染模式\n 光源的 类型 是指该光源是平行光还是其他类型的光源 光源的 渲染模式 是指该光源是否是 重要的 (Important)  前向渲染中，Unity 会根据场景中各个光源的设置以及光源对物体的影响程度 (距离远近、光源强度等)，对光源进行一个权重排序。然后 Unity 将一定数量的光源 逐像素处理，最多 4 个光源 逐顶点处理，剩下的光源 SH方式处理。\n其中的判断规则如下：\n 最亮的平行光逐像素处理 渲染模式设置为 Important 的光源，逐像素处理 渲染模式设置为 Not Important 的光源，逐顶点处理或者 SH 处理 若以上规则得到的逐像素光源的数量小于 Quality Setting 中的逐像素光源数量 (Pixel Light Count)，会有更多的光源以逐像素方式处理。  在哪进行光照计算 前向渲染有两种 Pass：Base Pass 和 Additional Pass。通常这两种进行标签和渲染设置以及常规光照计算如图所示：\n \n几点说明：\n 使用了 #pragma multi_compile_fwdbase 编译指令，以保证 Unity 可以为相应类型的 Pass 生成所需的 Shader 变种，这些变种会处理不同条件下的渲染逻辑，例如是否使用了光照贴图，当前处理哪种光源类型，是否开启了阴影等，同时 Unity 也会在背后声明相关的内置变量并传递到 Shader 中。 Base Pass 旁边的注释给出了 Base Pass 中支持的一些光照特性。例如在 Base Pass 中我们可以访问光照纹理（lightmap） Base Pass 中渲染的平行光默认是支持阴影的（如果开启了光源的阴影功能），而 Additional Pass 中渲染的光源在默认情况下是没有阴影效果的，即便我们在它的 Light 组件中设置了有阴影的 Shadow Type。但我们可以在 Additional Pass 中使用 #pragma multi_compile_fwdadd_fullshadows 代替 #pragma multi_compile_fwdadd 编译指令，为点光源和聚光灯开启阴影效果，但这需要在 Unity 内部使用更多的 Shader 变种。 环境光和自发光也是在 Base pass 中计算的。这是因为对于一个物体来说，环境光和自发光我们只希望计算一次即可，而如果我们在 Additional Pass 中计算这两种光照，就会造成叠加多次环境光和自发光，这不是我们想要的。 在 Additional Pass 的渲染设置中，我们还开启和设置了混合模式。这是因为我们希望每个 Additional Pass 可以与上一次的光照结果在帧缓存中进行叠加，从而得到最终有多个光照的渲染效果。如果我们没有开启和设置混合模式，那么 Additional Pass 的渲染结果会覆盖掉之前的渲染结果，看起来好像该物体只受该光源的影响。通常情况下，我们选择的混合模式是 Blend One One。 对于前向渲染来说，一个 UnityShader 通常会定义一个 Base Pass（Base Pass 也可以被定义多次，例如需要双面渲染的情况）以及一个 Additional Pass。一个 Base Pass 仅会执行一次（定义了多个 Base Pass 的情况除外），而一个 Additional Pass 会根据影响该物体的其他逐像素光源数目被多次调用，即每个逐像素光源会执行一次 Additional Pass。  上图给出的光照计算是 通常情况 下我们在每种Pass中进行的计算。实际上，渲染路径的设置用于告诉Unity该Pass在前向渲染路径中的位置，然后底层的渲染引擎会进行相关计算并填充一些内置变量（如_LightColor0等），如何使用这些内置变量进行计算完全取决于开发者的选择。例如我们完全可以利用 Unity 提供的内置变量在 Base Pass 中只进行逐顶点光照；同样，我们也可以完全在 Additional Pass 中按逐顶点的方式进行光照计算，不进行任何逐像素计算。\n内置的光照变量和函数 根据使用的渲染路径 (即 Pass 标签中 LightMode 的值)，Unity 会把不同的光照变量传递给 Shader，在 Unity5 中，对于前向渲染 (即 LightMode 为 ForwardBase 或 ForwardAdd) 来说，下表给出了我们可以在 Shader 中访问的光照变量。\n   名称 类型 描述     _LightColor0 float4 该 Pass 处理的逐像素光源的颜色。   _WorldSpaceLightPos0 float4 _WorldSpaceLightPos0.xyz 是该 Pass 处理逐像素光源的位置。若为平行光，那么 WorldSpaceLightPos0.w 为 0，其它光源类型 w 值为 1。   _LightMatrix0 float4x4 从世界空间到光源空间的变换矩阵，可以用于采样 cookie 和光源衰减（attenuation）纹理。   unity_4LightPosX0, unity_4LightPosY0, unity_4LightPosZ0 float4 仅用于 Base Pass，前 4 个非重要的点光源在世界空间中的位置。   unity_4LightAtten0 float4 仅用于 Base Pass，存储了前 4 个非重要的点光源的衰减因子。   unity_LightColor half4[4] 仅用于 Base Pass，存储了前 4 个非重要的点光源的颜色。    仅 可在前向渲染中可以使用的内置光照函数\n   函数名 描述     float3 WorldSpaceLightDir(float4 Mpos) 输入一个模型空间的顶点位置，返回世界空间中该点到光源的光照方向。内部实现使用了 UnityWorldSpaceLightDir 函数，没有归一化。   float3 UnityWorldSpaceLightDir(float4 Wpos) 输入一个世界空间的顶点位置，返回世界空间中该点到光源的光照方向。没有归一化。   float3 ObjSpaceLightDir(float4 Mpos) 输入一个模型空间的顶点位置，返回模型空间中该点到光源的光照方向。没有归一化。   float3 Shade4PointLights(\u0026hellip;) 计算四个点光源的光照，它的参数已经打包进矢量的光照数据，通常就是上个表格中的内置变量，如：unity_4LightAtten0、unity_LightColor等，前向渲染通常使用这个函数计算逐顶点光照。    延迟渲染路径 因解决前向渲染所带来的瓶颈问题而流行起来\n原理 除了前向渲染中使用的颜色缓冲和深度缓冲外，延迟渲染还会利用额外的缓冲区，这些缓冲也被称为 G 缓冲（G-buffer），其中G是英文Geometry的缩写。G缓冲区存储了我们所关心的表面（通常指的是离摄像机最近的表面）的其他信息，例如该表面的法线、位置、用于光照计算的材质属性等。\n延迟渲染主要包含量两个Pass：\n 在第一个Pass中，我们不进行任何光照计算，而仅仅计算哪片片元是可见的，这主要是通过深度缓冲区技术来实现，当发现一个片元是可见的，我们就把它的相关信息存储到 G 缓冲区中。 在第二个Pass中，我们利用 G 缓冲区的各个片元信息，例如表面法线、视角方向、漫反射系数等，进行真正的光照计算。  延迟渲染的伪代码如下：\nPass1{ // 延迟渲染第一个 Pass 不用于计算光照，而是收集所有的深度信息，法线信息等 // 在第二个Pass中进行实际的光照计算，因而也被称为延迟渲染。 for(each primitive in this model){ for(each fragment covered by this primitve){ if(failed in depth test){ // 如果没有通过深度测试，说明该片元是不可见的 discard; }else{ writeGBuffer(materialInfo,pos,normal,lightDir,viewDir); // 如果该片元可见，就把需要的信息存储到G缓冲中去 } } } } Pass2{ for(each pixel in the screen){ if(the pixel is valid){ // 如果该像素有效，那么就读取它对应的G缓冲中的信息 readGBuffer(pixel,materialInfo,pos,normal,lightDir,viewDir); // 在此处计算光照 float4 color = Shading(materialInfo,pos,normal,lightDir,viewDir); writeFrameBuffer(); // 更新帧缓存 } } }  延迟渲染的效率不依赖于场景的复杂度，而是和我们使用的屏幕空间大小有关。这是因为，我们需要的信息都存储在缓冲区中，而这些缓冲区可以理解成是一张张2D图像，我们的计算实际上就是在这些图像空间中进行的。\n Unity 中的延迟渲染 延迟渲染路径，它最适合在场景中光源数目很多、如果使用前向渲染会造成性能瓶颈的情况下使用。而且，延迟渲染路径中的每个光源都可以按照逐像素的方式处理。但是，延迟渲染也有一些缺点。\n 不支持真正的抗锯齿（anti-aliasing）功能。 不能处理半透明物体 对显卡有一定要求。如果要使用延迟渲染的话，显卡必须支持 MRT（Multiple Render Targets）、Shader Mode3.0 及以上、深度渲染纹理以及双面的模板缓冲。  unity 要求提供两个 Pass  第一个Pass用于渲染 G 缓冲。在这个 Pass 中，我们会把物体的漫反射颜色、高光反射颜色、平滑度、法线、自发光和深度等信息渲染到屏幕空间的 G 缓冲区中。对于每个物体来说，这个 Pass 仅会执行一次。 第二个 Pass 用于计算真正的光照模型。这个 Pass 会使用上一个 Pass 中渲染的数据来计算最终的光照颜色，再存储到帧缓冲中。   注意：当在第二个 Pass 计算光照时，默认情况下仅可以使用 Unity 内置的 Standard 光照模型。如果我们想要使用其它的光照模型，就需要替换掉原有的 Internal-DefferedShading.shader 文件。\n 默认的 G 缓冲区 注意，不同 Unity 版本的渲染纹理存储内容会有所不同，缓冲区包含了以下几个渲染纹理（Render Texture，RT）\n RT0：格式是 AGRB32，RGB 通道用于存储漫反射颜色，A通道没有被使用】 RT1：格式是 AGRB32，RGB 通道用于存储高光反射颜色，A通道用于存储高光反射的指数部分。 RT2：格式是 ARGB2101010，RGB 通道用于存储法线，A通道没有使用 RT3：格式是 ARGB32（非HDR）或 ARGBHalf（HDR），用于存储自发光 +lightmap+ 反射探针（reflection probes）  可访问的内置变量和函数 下表给出了处理延迟渲染路径可以使用的光照变量。这些变量都可以在UnityDefferedLibrary.cginc文件中找到它们的声明。\n   名称 类型 描述     _LightColor float4 光源颜色   _LightMatrix0 float4x4 从世界空间到光源空间的变换矩阵，可以用于采样 cookie 和光强衰减纹理    Unity 光源类型 Unity 中提供了4种光源类型：平行光、点光源、聚光灯和面光源（area light）。面光源尽在烘焙时才可发挥作用，因此我们不讨它。\n光源的五大属性：位置、方向、颜色、强度、衰减。\n前向渲染中处理不同的光源类型 代码中使用了 Blinn-Phong 光照模型，并定义了 Base Pass 和 Additional Pass 来处理多个光源，原理代码如下：\nShader \u0026#34;Unity Shaders Book/Chapter 9/Forward Rendering\u0026#34; { Properties { _Diffuse (\u0026#34;Diffuse\u0026#34;, Color) = (1, 1, 1, 1) _Specular (\u0026#34;Specular\u0026#34;, Color) = (1, 1, 1, 1) _Gloss (\u0026#34;Gloss\u0026#34;, Range(8.0, 256)) = 20 } SubShader { Tags { \u0026#34;RenderType\u0026#34; = \u0026#34;Opaque\u0026#34; } // BasePass Pass { // 环境光和第一像素光(方向光) Pass Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ForwardBase\u0026#34; } CGPROGRAM // 显然需要添加此声明 #pragma multi_compile_fwdbase #pragma vertex vert #pragma fragment frag #include \u0026#34;Lighting.cginc\u0026#34; fixed4 _Diffuse; fixed4 _Specular; float _Gloss; struct a2v { float4 vertex: POSITION; float3 normal: NORMAL; }; struct v2f { float4 pos: SV_POSITION; float3 worldNormal: TEXCOORD0; float3 worldPos: TEXCOORD1; }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; return o; } fixed4 frag(v2f i): SV_Target { fixed3 worldNormal = normalize(i.worldNormal); // _WorldSpaceLightPos0平行光方向 fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); // 计算场景中的环境光 fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; // _LightColor0平行光颜色和强度 fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb * max(0, dot(worldNormal, worldLightDir)); fixed3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.worldPos.xyz); fixed3 halfDir = normalize(worldLightDir + viewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(worldNormal, halfDir)), _Gloss); // 平行光的衰减为1 fixed atten = 1.0; return fixed4(ambient + (diffuse + specular) * atten, 1.0); } ENDCG } // Additional Pass Pass { // 其他像素光源Pass Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ForwardAdd\u0026#34; } // 开启混合模式用于叠加光照 Blend One One CGPROGRAM // 显然需要添加此声明 #pragma multi_compile_fwdadd #pragma vertex vert #pragma fragment frag #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; fixed4 _Diffuse; fixed4 _Specular; float _Gloss; struct a2v { float4 vertex: POSITION; float3 normal: NORMAL; }; struct v2f { float4 pos: SV_POSITION; float3 worldNormal: TEXCOORD0; float3 worldPos: TEXCOORD1; }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; return o; } fixed4 frag(v2f i): SV_Target { fixed3 worldNormal = normalize(i.worldNormal); // 用于判断当前光源是否是平行光 #ifdef USING_DIRECTIONAL_LIGHT // 获取平行光的方向 fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); #else // 通过点光源（或聚光灯）的位置减去世界空间下顶点位置，获得光的方向 fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz - i.worldPos.xyz); #endif fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb * max(0, dot(worldNormal, worldLightDir)); fixed3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.worldPos.xyz); fixed3 halfDir = normalize(worldLightDir + viewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(worldNormal, halfDir)), _Gloss); // 判断当前光源是否是平行光 #ifdef USING_DIRECTIONAL_LIGHT // 平行光衰减值为1 fixed atten = 1.0; #else // 如果是点光源，光的衰减值计算 #if defined(POINT) float3 lightCoord = mul(unity_WorldToLight, float4(i.worldPos, 1)).xyz; fixed atten = tex2D(_LightTexture0, dot(lightCoord, lightCoord).rr).UNITY_ATTEN_CHANNEL; // 如果是聚光灯，光的衰减值计算 #elif defined(SPOT) float4 lightCoord = mul(unity_WorldToLight, float4(i.worldPos, 1)); fixed atten = (lightCoord.z \u0026gt; 0) * tex2D(_LightTexture0, lightCoord.xy / lightCoord.w + 0.5).w * tex2D(_LightTextureB0, dot(lightCoord, lightCoord).rr).UNITY_ATTEN_CHANNEL; #else fixed atten = 1.0; #endif #endif return fixed4((diffuse + specular) * atten, 1.0); } ENDCG } } FallBack \u0026#34;Specular\u0026#34; }  上诉代码只是讲述原理，不可用于真正的项目中\n 渲染顺序 我们知道，如果逐像素光源的数目很多的话，该物体的 Additional Pass 就会被调用多次，影响性能，我们可以通过把光源的 Render Mode 设为 Not Important 来告诉Unity，我们不希望把该光源当成逐像素处理。\nUnity 的光照衰减 Unity 中，如果是平行光的话，衰减值为 1.0。如果是其它的光源类型，那么处理更复杂一些。尽管我们可以使用数学表达式来计算给顶点相对于点光源和聚光灯的衰减，但这些计算往往涉及开根号、除法等计算量较大的操作，因此 Unity 选择了使用一张纹理作为查找表（Lookup Table，LUT），以在片元着色器中得到光源的衰减。我们首先得到光源空间下的坐标，然后利用该坐标对衰减纹理进行采样得到衰减值。\n这样的好处在于，计算衰减不依赖于数学公式的复杂性，我们只要使用一个参数值去纹理中采样即可。但使用纹理查找来计算衰减也有一些弊端：\n 需要预处理得到采样纹理，而且纹理的大小也会影响衰减的精度 不直观，同时也不方便，因此一旦把数据存储到查找表中，我们就无法使用其它数学公式来计算衰减  但由于这种方法可以在一定程度上提升性能，而且得到的效果在大部分情况下都是良好的，因此 Unity 默认的就是使用这种纹理查找方式来计算逐像素的点光源和聚光灯的衰减的。\n用于光照衰减的纹理 Unity 在内部使用一张叫做 _LightTexture0 的纹理来计算光源的衰减，需要注意的是，如果我们对光源使用了cookie（相当于一张阴影贴纸），那么衰减查找纹理就成了 _LightTextureB0 ，但这不是重点。在这张纹理上，（0，0）点表明了与光源位置重合点的衰减（就是最近点），（1，1）表明了光源涉及到的范围的最远点的衰减。\n为了对 _LightTexture0 纹理采样得到给定点到该光源的衰减值，我们首先需要得到该点在光源空间中的位置，这是通过 _LightMatrix0 变换矩阵得到的。在前面我们已经知道 _LightMatrix0 可以把顶点从世界空间变换到光源空间。因此，我们只需把 _LightMatrix0 和世界空间中的顶点坐标相乘即可得到光源空间中的相应位置。\nfloat3 lightCoord = mul(_LightMatrix0, float4(i.worldPosition,1)).xyz; 然后，我们可以使用这个坐标模的平方对衰减纹理进行采样，得到衰减值：\nfixed atten = tex2D(_LightTexture0, dot(lightCoord，lightCoord).rr).UNITY_ATTEN_CHANNEL; 可以发现，在上面的代码中，我们使用了光源空间中顶点距离的平方（通过 dot 函数来得到）来对纹理采样，之所以没有使用距离值来采样是因为这种方法可以避免开方操作。最后，我们使用宏 UNITY_ATTEN_CHANNEL 来得到衰减纹理中衰减值所在的分量，以得到最终的衰减值。\n使用数学公式计算 尽管纹理衰减的方法可以减少计算衰减时的复杂度，但有时我们希望可以在代码中利用公式来计算光源的衰减。例如下面的代码可以计算光源的线性衰减。\nfloat distance = length(_WorldSpaceLightPos.xyz - i.worldPosition.xyz); atten = 1.0 / distance;//线性衰减 Unity 没有在文档中给出内置衰减计算的说明。\nUnity 阴影计算 阴影实现 (shadowmap技术) 在实时渲染中，我们最常使用是一种名为 Shadow Map 的技术。这种技术理解起来非常简单，它会首先把摄像机的位置放在与光源重合的位置上，那么场景中该光源的阴影区域就是那些摄像机看不到的地方，而 Unity 使用的就是这样的技术。 在前向渲染路径中，如果场景中最重要的平行光开启了阴影，Unity 就会为该光源计算它的 阴影映射纹理（shadowmap） 。这张阴影映射纹理本质上也是一张 深度图，它记录了从该光源位置出发、能看到的场景中距离它最近的表面位置（深度信息）。\n距离阴影映射纹理最近的表面位置 在计算阴影映射纹理时，我们如何判定距离它最近的表面位置呢？\n  一种方法是，先把摄像机放置到光源位置上，然后按正常的渲染流程，即调用 Base Pass 和 Additional Pass 来更新深度信息，得到阴影映射纹理。\n但这种方法会对性能造成一定的浪费，因为实际上我们仅仅需要深度信息而已，而 Base Pass 和 Additional Pass 往往会涉及很多复杂的光照模型计算。\n  Unity 选择使用一个额外的 Pass 来专门更新光源的阴影映射纹理，这个 Pass 就是 LightMode 标签被设置为 ShadowCaster 的 Pass。这个 Pass 的渲染目标不是帧缓存，而是 阴影映射纹理（或深度纹理）。\nUnity 首先把摄像机放置到光源位置上，然后调用该 Pass，通过对顶点变换后得到光源空间下的位置，并据此来输出深度信息到阴影映射纹理中。\n因此，当开启了光源的阴影效果后，底层渲染引擎首先会在当前渲染物体的 UnityShader 中找到 LightMode 为 ShadowCaster 的 Pass，如果没有，它就会在 Fallback 指定的 Unity Shader 中继续寻找，如果仍然没有找到，该物体就无法向其它物体投射阴影（但它仍然可以接收来自其它物体的阴影）。当找到一个 LightMode 为 ShadowCaster 的 Pass 后，Unity 会使用该 Pass 来更新光源的阴影映射纹理。\n阴影映射纹理实现 阴影映射纹理实现是如何实现的呢？\n 在传统的阴影映射纹理的实现中，我们会在正常渲染的Pass中把顶点位置变换到光源空间下，以得到它在光源空间中的三维位置信息。然后我们使用xy分量对阴影映射纹理进行采样，得到阴影映射纹理中该位置的深度信息。如果该深度值小于该顶点的深度值（通常由z分量得到），那么说明该点位于阴影中。 在 Unity5 中，Unity 使用了不同于这种传统的阴影采样技术，即 屏幕空间的阴影映射技术（Screenspace Shadow Map） 。屏幕空间的阴影映射原本是延迟渲染中产生阴影的方法。   需要注意的是，并不是所有平台的 Unity 都会使用这种技术。这是因为，屏幕空间的阴影映射需要显卡支持 MRT，而有些移动平台不支持这种特性。\n   Unity 中阴影映射技术具体实现 当使用了屏幕空间的阴影映射技术时，Unity 首先会通过调用 LightMode 为 ShadowCaster 的 Pass 来得到可投射阴影的光源的阴影映射纹理以及摄像机的深度纹理。然后，根据光源的阴影映射纹理和摄像机的深度纹理来得到屏幕空间的阴影图。\n如果摄像机的深度图中记录的表面深度大于转换到阴影映射纹理的深度值，就说明该表面虽然是可见的，但却处于该光源的阴影中。通过这样的方式，阴影图就包含了屏幕空间中所有有阴影的区域。\n如果我们想要一个物体接收来自其它物体的阴影，只需要在 Shader 中对阴影图进行采样。由于阴影图是屏幕空间下的，因此我们首先需要把表面坐标从模型空间变换到屏幕空间中，然后使用这个坐标对阴影图进行采样即可。\n小结  如果我们想要一个物体接收来自其它物体的阴影，就必须在 Shader 中对阴影映射纹理（包括屏幕空间的阴影图）进行采样，把采样结果和最后光照结果相乘来产生阴影效果。 如果我们想要一个物体向其它物体投射阴影，就必须把该物体加入到光源的阴影映射纹理的计算中，从而让其它物体对阴影映射纹理采样时可以得到该物体的相关信息。  Unity 中，这个过程是通过为该物体执行 LightMode 为 ShadowCaster 的 Pass 来实现的。如果使用了屏幕空间的投影映射技术，Unity 还会使用这个 Pass 产生一张摄像机的深度纹理。\n不透明物体阴影 为了让场景中可以产生阴影，首先需要让平行光可以收集阴影信息。这需要在光源的 Light 组件中开启阴影。\n让物体投射阴影 Unity 中，我们可以选择是否让一个物体投射或接收阴影。通过设置 Mesh Renderer 组件中的 Cast Shadows 和 Receive Shadows 属性来实现的。\nCast Shadows 可以被设置为开启（On）或关闭（Off）。若开启 Cast Shadows 属性，Unity 会把该物体加入光源的阴影映射纹理的计算中，从而让其它物体在对阴影映射纹理采样时可以得到该物体的相关信息，也就是该物体可以向其他物体投射阴影。\nReceive Shadows 则可以选择是否让物体接收来自其它物体的阴影。若没有开启 Receive Shadows，那么当我们调用 Unity 的内置宏和变量计算阴影时，这些宏通过判断该物体没有开启接收阴影的功能，就不会在内部为我们计算阴影。\n让物体接受阴影   我们在Base Pass中包含进一个新的内值文件，用于获取我们后续要使用到的宏：\n#include \u0026#34;AutoLight.cginc\u0026#34;   SHADOW_COORDS、TRANSFER_SHADOW 和 SHADOW_ATTENUATION 是计算阴影时的 “三剑客” 。我们可以在 AutoLight.cginc 中找到它们的声明。\n  在前向渲染中，宏 SHADOW_COORDS 实际上就是声明了一个名为 _ShadowCoord 的阴影纹理坐标变量。而 TRANSFER_SHADOW 的实现会根据平台不同而有所差异。如果当前平台可以使用屏幕空间的阴影映射技术（通过判断是否定义了 UNITY_NO_SCREENSPACE_SHADOWS 来得到），TRANSFER_SHADOW 会调用内置的 ComputePos 函数来计算 _ShadowCoord；如果该平台不支持屏幕空间的阴影映射技术，就会使用传统的阴影映射技术，TRANSFER_SHADOW 会把顶点坐标从模型空间变换到光源空间后存储到 _ShadowCoord 中。然后 SHADOW_ATTENUATION 负责使用 _ShadowCoord 对相关纹理进行采样，得到阴影信息。\n 这些宏会使用上下文变量来进行相关计算，例如 TRANSFER_SHADOW 会使用 v.vertex 或 a.pos 来计算坐标，因此为了能够让这些宏正确工作，我们需要保证自定义的变量名和这些宏中使用的变量名相匹配。我们需要保证：a2f 结构体中顶点坐标变量名必须是 vertex，顶点着色器的输出结构体 v2f 必须命名为 v，且 v2f 中的顶点位置变量必须命名为 pos。\n Shader \u0026#34;Unity Shaders Book/Chapter 9/Shadow\u0026#34; { Properties { _Diffuse (\u0026#34;Diffuse\u0026#34;, Color) = (1, 1, 1, 1) _Specular (\u0026#34;Specular\u0026#34;, Color) = (1, 1, 1, 1) _Gloss (\u0026#34;Gloss\u0026#34;, Range(8.0, 256)) = 20 } SubShader { Tags { \u0026#34;RenderType\u0026#34; = \u0026#34;Opaque\u0026#34; } Pass { // 环境光和第一像素光(方向光)Pass Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ForwardBase\u0026#34; } CGPROGRAM // 显然需要添加此声明 #pragma multi_compile_fwdbase #pragma vertex vert #pragma fragment frag // 需要这些文件获得内置宏 #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; fixed4 _Diffuse; fixed4 _Specular; float _Gloss; struct a2v { float4 vertex: POSITION; float3 normal: NORMAL; }; struct v2f { float4 pos: SV_POSITION; float3 worldNormal: TEXCOORD0; float3 worldPos: TEXCOORD1; // 这个宏用于声明一个作用于阴影纹理采样的坐标 SHADOW_COORDS(2) }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; // 这个宏用于将阴影坐标传递到像素着色器 TRANSFER_SHADOW(o); return o; } fixed4 frag(v2f i): SV_Target { fixed3 worldNormal = normalize(i.worldNormal); fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb * max(0, dot(worldNormal, worldLightDir)); fixed3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.worldPos.xyz); fixed3 halfDir = normalize(worldLightDir + viewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(worldNormal, halfDir)), _Gloss); fixed atten = 1.0; // 使用宏坐标来采样阴影贴图 fixed shadow = SHADOW_ATTENUATION(i); return fixed4(ambient + (diffuse + specular) * atten * shadow, 1.0); } ENDCG } Pass { // 其他像素光源Pass Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ForwardAdd\u0026#34; } Blend One One CGPROGRAM // 显然需要添加此声明 #pragma multi_compile_fwdadd // 使用下面的line为点光源和聚光灯添加阴影 //\t#pragma multi_compile_fwdadd_fullshadows #pragma vertex vert #pragma fragment frag #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; fixed4 _Diffuse; fixed4 _Specular; float _Gloss; struct a2v { float4 vertex: POSITION; float3 normal: NORMAL; }; struct v2f { float4 position: SV_POSITION; float3 worldNormal: TEXCOORD0; float3 worldPos: TEXCOORD1; }; v2f vert(a2v v) { v2f o; o.position = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; return o; } fixed4 frag(v2f i): SV_Target { fixed3 worldNormal = normalize(i.worldNormal); #ifdef USING_DIRECTIONAL_LIGHT fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); #else fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz - i.worldPos.xyz); #endif fixed3 diffuse = _LightColor0.rgb * _Diffuse.rgb * max(0, dot(worldNormal, worldLightDir)); fixed3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.worldPos.xyz); fixed3 halfDir = normalize(worldLightDir + viewDir); fixed3 specular = _LightColor0.rgb * _Specular.rgb * pow(max(0, dot(worldNormal, halfDir)), _Gloss); #ifdef USING_DIRECTIONAL_LIGHT fixed atten = 1.0; #else float3 lightCoord = mul(unity_WorldToLight, float4(i.worldPos, 1)).xyz; fixed atten = tex2D(_LightTexture0, dot(lightCoord, lightCoord).rr).UNITY_ATTEN_CHANNEL; #endif return fixed4((diffuse + specular) * atten, 1.0); } ENDCG } } FallBack \u0026#34;Specular\u0026#34; } 统一管理光照和衰减 在前面，我们已经讲过如何在 UnityShader 的前向渲染路径中计算光照衰减——在 Base Pass 中，平行光的衰减因子总是等于1，而在 Additional Pass 中，我们需要判断该 Pass 处理的光源类型，再使用内置变量和宏计算衰减因子。那么是不是有一个方法可以同时计算两个信息呢？好消息是，Unity 在 Shader 里提供了这样的功能，这主要是通过内置的 UNITY_LIGHT_ATTENUATION 宏来实现的。\nUNITY_LIGHT_ATTENUATION 是 Unity 内置的用于计算光照衰减和阴影的宏，我们可以在内置的 AutoLight.cginc 里找到它们的相关声明。它接收3个参数：\n atten：光照衰减和阴影值相乘后的结果存储到第一个参数中。注意到，我们并没有在代码中声明第一个参数atten，这是因为 UNITY_LIGHT_ATTENUATION 会帮我们声明这个变量。 结构体 v2f，这个参数会传递给 SHADOW_ATTENUATION ，用来计算阴影值。 世界空间的坐标，正如我们在前面讲的那样，这个参数会用于计算光源空间下的坐标，再对光照衰减纹理采样得到的光照衰减。  Shader \u0026#34;Unlit/ShadowRecive\u0026#34; { Properties { _Color(\u0026#34;Color\u0026#34;, Color) = (1,1,1,1) _Specualr(\u0026#34;Specualr\u0026#34;, Color) = (1,1,1,1) _SpecularPower(\u0026#34;SpecularPower\u0026#34;, Range(8.0, 256)) = 20 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } Pass { Tags {\u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34;} CGPROGRAM #pragma multi_compile_fwdbase #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; float4 _Color; float4 _Specualr; float _SpecularPower; struct appdata { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; float3 worldPos : TEXCOORD1; SHADOW_COORDS(2) }; v2f vert (appdata v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; TRANSFER_SHADOW(o); return o; } fixed4 frag (v2f i) : SV_Target { half3 worldNormal = normalize(i.worldNormal); half3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); half3 halfDir = normalize(worldLightDir + worldViewDir); half NdotL = max(0.0, dot(worldNormal, worldLightDir)); half NdotV = max(0.0, dot(worldNormal, halfDir)); half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; half3 diffuse = _LightColor0.rgb * _Color.rgb * NdotL; half3 specular = _LightColor0.rgb * _Specualr.rgb * pow(NdotV, _SpecularPower); UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); return half4(ambient + (diffuse + specular) * atten, 1.0); } ENDCG } Pass { Tags {\u0026#34;LightMode\u0026#34;=\u0026#34;ForwardAdd\u0026#34;} Blend One One CGPROGRAM #pragma multi_compile_fwdadd #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; float4 _Color; float4 _Specualr; float _SpecularPower; struct appdata { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; float3 worldPos : TEXCOORD1; SHADOW_COORDS(2) }; v2f vert (appdata v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; TRANSFER_SHADOW(o); return o; } fixed4 frag (v2f i) : SV_Target { half3 worldNormal = normalize(i.worldNormal); half3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); half3 halfDir = normalize(worldLightDir + worldViewDir); half NdotL = max(0.0, dot(worldNormal, worldLightDir)); half NdotV = max(0.0, dot(worldNormal, halfDir)); half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; half3 diffuse = _LightColor0.rgb * _Color.rgb * NdotL; half3 specular = _LightColor0.rgb * _Specualr.rgb * pow(NdotV, _SpecularPower); UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); return half4((diffuse + specular) * atten, 1.0); } ENDCG } } Fallback \u0026#34;Specular\u0026#34; } 由于我们使用了 UNITY_LIGHT_ATTENUATION，我们的 Base Pass 和 Additional Pass 的代码得以统一，我们不需要在 Base Pass 里单独处理阴影，也不需要在 Additional Pass 中判断光源类型来处理光照衰减，一切都只需要通过 UNITY_LIGHT_ATTENUATION 来完成即可。\n如果我们希望可以在 Additional Pass 中添加阴影效果，就需要使用 #pragma multi_compile_fwdadd_fullshadows 编译指令来代替 Additional Pass 中的 #pragma multi_compile_fwdadd 指令。这样一来，Unity 也会为这些额外的逐像素光源计算阴影，并传递给 Shader。\n透明物体的阴影 透明度测试的阴影 透明度测试的处理比较简单，但如果我们仍然直接使用 VertexLit、Diffuse、Specular 等做为回调，往往无法得到正确的阴影。这是因为透明度测试需要在片元着色器中舍弃某些片元，而 Transparent/Cutout/VertexLit 中的阴影投射纹理并没有进行这样的操作。\n我们采用和透明度测试时同样的 Shader 代码进行改造：\n  添加需要的头文件：\n#include \u0026quot;Lighting.cginc\u0026quot; #include \u0026quot;AutoLight.cginc\u0026quot;   在 v2f 中使用内置宏 SHADOW_COORDS 声明阴影纹理坐标：\n注意到，由于我们已经占用了3个插值寄存器（使用 TEXCOORD0、TEXCOORD1 和 TEXCOORD2 修饰的变量），因此 SHADOW_COORDS 传入的参数是3，这意味着阴影纹理坐标将占用第四个插值寄存器 TEXCOORD3。\nstruct v2f{ float4 pos:SV_POSITION; float3 worldNormal:TEXCOORD0; float3 worldPos:TEXCOORD1; float2 uv:TEXCOORD2; SHADOW_COORDS(3) };   在顶点着色器中使用内值宏 TRANSFER_SHADOW 计算阴影纹理坐标后传递给片元着色器\n  在片元着色器中，使用内置宏 UNITY_LIGHT_ATTENUATION 计算阴影和光照衰减\n  这次，我们更改它的 Fallback，使用 Transparent/Cutout/VertexLit 作为它的回调 Shader\n  Shader \u0026#34;Unlit/TransparentVetexLit\u0026#34; { Properties { _MainTex (\u0026#34;MainTex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Color(\u0026#34;Color\u0026#34;, Color) = (1,1,1,1) _Specular(\u0026#34;Specular Color\u0026#34;, Color) = (1,1,1,1) _SpecualrPower(\u0026#34;Specualr Power\u0026#34;, Range(8.0, 256)) = 20 _Cutoff(\u0026#34;CutOff\u0026#34;, Range(0.0, 1)) = 0.2 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } Pass { Tags {\u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34;} Cull Off CGPROGRAM #pragma multi_compile_fwdbase #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; float4 _Color; float4 _Specular; float _SpecualrPower; float _Cutoff; struct appdata { float4 vertex : POSITION; float2 texcoord : TEXCOORD0; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float2 uv : TEXCOORD0; float3 worldNormal : TEXCOORD1; float3 worldPos : TEXCOORD2; SHADOW_COORDS(3) }; v2f vert (appdata v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; o.uv = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; TRANSFER_SHADOW(o); return o; } fixed4 frag (v2f i) : SV_Target { half3 worldNormal = normalize(i.worldNormal); half3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); half3 halfDir = normalize(worldLightDir + worldViewDir); half3 NdotL = max(0.0, dot(worldNormal, worldLightDir)); half3 NdotV = max(0.0, dot(worldNormal, halfDir)); half4 albedo = tex2D(_MainTex, i.uv); half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo.rgb; half3 diffuse = _LightColor0.rgb * albedo.rgb * _Color.rgb * NdotL; half3 specular = _LightColor0.rgb * _Specular.rgb * pow(NdotV, _SpecualrPower); clip(albedo.a - _Cutoff); UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); return half4(ambient + (diffuse + specular) * atten, 1.0); } ENDCG } Pass { Tags {\u0026#34;LightMode\u0026#34;=\u0026#34;ForwardAdd\u0026#34;} Cull Off Blend One One CGPROGRAM #pragma multi_compile_fwdadd #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; float4 _Color; float4 _Specular; float _SpecualrPower; float _Cutoff; struct appdata { float4 vertex : POSITION; float2 texcoord : TEXCOORD0; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float2 uv : TEXCOORD0; float3 worldNormal : TEXCOORD1; float3 worldPos : TEXCOORD2; SHADOW_COORDS(3) }; v2f vert (appdata v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; o.uv = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; TRANSFER_SHADOW(o); return o; } fixed4 frag (v2f i) : SV_Target { half3 worldNormal = normalize(i.worldNormal); half3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); half3 halfDir = normalize(worldLightDir + worldViewDir); half3 NdotL = max(0.0, dot(worldNormal, worldLightDir)); half3 NdotV = max(0.0, dot(worldNormal, halfDir)); half4 albedo = tex2D(_MainTex, i.uv); half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo.rgb; half3 diffuse = _LightColor0.rgb * albedo.rgb * _Color.rgb * NdotL; half3 specular = _LightColor0.rgb * _Specular.rgb * pow(NdotV, _SpecualrPower); clip(albedo.a - _Cutoff); UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); return half4((diffuse + specular) * atten, 1.0); } ENDCG } } // Fallback \u0026#34;VertexLit\u0026#34; Fallback \u0026#34;Transparent/Cutout/VertexLit\u0026#34; } 透明度混合的阴影 事实上，所有内置的透明度混合的 Unity Shader，如 Transparent/VertexLit 等，都没有包含阴影投射的 Pass。这意味着，这些半透明物体不会参与深度图和阴影映射纹理的计算，也就是说，它们不会向其他物体投射阴影，同样它们也不会接受来自其它物体的阴影 。\n由于透明度混合需要关闭深度写入，由此带来的问题也影响了阴影的生成。总体来说，要想为这些透明半透明物体产生正确的阴影，需要在每个光源空间下仍然严格按照从后往前的顺序进行渲染，这会让阴影处理变得非常复杂，而且会影响性能。因此，在 Unity 中，所有内置的半透明 Shader 是不会产生任何阴影效果的。\n强制阴影效果 我们可以使用一些 dirty trick 来强制为半透明物体生成阴影，这可以通过把它们的 Fallback 设置为 VertexLit、Diffuse 这些不透明物体使用的 UnityShader，这样 Unity 就会在它的 Fallback 找到一个阴影投射的Pass，然后我们可以通过物体的 Mesh Render 组件上的 Cast Shadows 和 Receive Shadows 选项来控制是否需要向其他物体投射或接收阴影。\n代码参考实现：(这一串代码并没用，而且还不能投射阴影，通过修改后也只能投射阴影不能接受阴影)\nShader \u0026#34;Unity Shaders Book/Chapter 9/Alpha Blend With Shadow\u0026#34; { Properties { _Color (\u0026#34;Color Tint\u0026#34;, Color) = (1, 1, 1, 1) _MainTex (\u0026#34;Main Tex\u0026#34;, 2D) = \u0026#34;white\u0026#34; { } _AlphaScale (\u0026#34;Alpha Scale\u0026#34;, Range(0, 1)) = 1 } SubShader { Tags { \u0026#34;Queue\u0026#34; = \u0026#34;Transparent\u0026#34; \u0026#34;IgnoreProjector\u0026#34; = \u0026#34;True\u0026#34; \u0026#34;RenderType\u0026#34; = \u0026#34;Transparent\u0026#34; } Pass { Tags { \u0026#34;LightMode\u0026#34; = \u0026#34;ForwardBase\u0026#34; } ZWrite Off Blend SrcAlpha OneMinusSrcAlpha CGPROGRAM #pragma multi_compile_fwdbase #pragma vertex vert #pragma fragment frag #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; fixed4 _Color; sampler2D _MainTex; float4 _MainTex_ST; fixed _AlphaScale; struct a2v { float4 vertex: POSITION; float3 normal: NORMAL; float4 texcoord: TEXCOORD0; }; struct v2f { float4 pos: SV_POSITION; float3 worldNormal: TEXCOORD0; float3 worldPos: TEXCOORD1; float2 uv: TEXCOORD2; SHADOW_COORDS(3) }; v2f vert(a2v v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; o.uv = TRANSFORM_TEX(v.texcoord, _MainTex); // 传递阴影坐标到像素着色器 TRANSFER_SHADOW(o); return o; } fixed4 frag(v2f i): SV_Target { fixed3 worldNormal = normalize(i.worldNormal); fixed3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); fixed4 texColor = tex2D(_MainTex, i.uv); fixed3 albedo = texColor.rgb * _Color.rgb; fixed3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; fixed3 diffuse = _LightColor0.rgb * albedo * max(0, dot(worldNormal, worldLightDir)); // UNITY_LIGHT_ATTENUATION)不仅可以计算衰减，还可以计算阴影信息 UNITY_LIGHT_ATTENUATION(atten, i, i.worldPos); return fixed4(ambient + diffuse * atten, texColor.a * _AlphaScale); } ENDCG } } FallBack \u0026#34;Transparent/VertexLit\u0026#34; // 或用force应用阴影 //\tFallBack \u0026#34;VertexLit\u0026#34; } 可使用的标准光照着色器 在这里，这个 Shader 包含了对法线纹理、多光源、光照衰减和阴影的相关处理。\nShader \u0026#34;Unlit/DiffuseBump\u0026#34; { Properties { _MainTex (\u0026#34;MainTex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Color(\u0026#34;Color\u0026#34;, Color) = (1,1,1,1) _BumpTex(\u0026#34;BumpTex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Specular(\u0026#34;Specular\u0026#34;, Color) = (1,1,1,1) _SpecualrPower(\u0026#34;Specualr Power\u0026#34;, Range(8.0, 256)) = 20 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } Pass { Tags {\u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34;} CGPROGRAM // Base Pass 平行光逐像素渲染 #pragma multi_compile_fwdbase #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; sampler2D _BumpTex; float4 _BumpTex_ST; float4 _Color; float4 _Specular; float _SpecualrPower; struct appdata { float4 vertex : POSITION; float4 texcoord : TEXCOORD0; float4 tangent : TANGENT; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float4 uv : TEXCOORD0; float4 TtoW0 : TEXCOORD1; float4 TtoW1 : TEXCOORD2; float4 TtoW2 : TEXCOORD3; SHADOW_COORDS(4) // 声明一个光源的阴影映射纹理 }; v2f vert (appdata v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv.xy = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; // 应用 _MainTex 的缩放和平移 o.uv.zw = v.texcoord.xy * _BumpTex_ST.xy + _BumpTex_ST.zw; // 应用 _BumpTex 的缩放和平移 half3 worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; // 顶点位置转到世界空间 half3 worldNormal = UnityObjectToWorldNormal(v.normal); // 顶点法线转到世界空间 half3 worldTangent = UnityObjectToWorldDir(v.tangent.xyz); // 顶点切线转到世界空间 half3 worldBinormal = cross(worldNormal, worldTangent) * v.tangent.w; // 计算得到世界空间下的顶点副切线 o.TtoW0 = float4(worldTangent.x, worldBinormal.x, worldNormal.x, worldPos.x); o.TtoW1 = float4(worldTangent.y, worldBinormal.y, worldNormal.y, worldPos.y); o.TtoW2 = float4(worldTangent.z, worldBinormal.z, worldNormal.z, worldPos.z); TRANSFER_SHADOW(o); // 计算声明过的阴影映射纹理 return o; } fixed4 frag (v2f i) : SV_Target { half3 worldPos = half3(i.TtoW0.w, i.TtoW1.w, i.TtoW2.w); // 世界空间下的坐标 half3 worldLightDir = normalize(UnityWorldSpaceLightDir(worldPos)); // 世界空间下的光源方向 half3 worldViewDir = normalize(UnityWorldSpaceViewDir(worldPos)); // 世界空间下的视角方向 half3 halfDir = normalize(worldLightDir + worldViewDir); // 半角方向 half3 bump = UnpackNormal(tex2D(_BumpTex, i.uv.zw)); // 采样法线贴图 bump = normalize(half3(dot(i.TtoW0.xyz, bump), dot(i.TtoW1.xyz, bump), dot(i.TtoW2.xyz, bump)));// 法线坐标从切线空间转到世界空间 half NdotL = max(0.0, dot(bump, worldLightDir)); // lambert half NdotV = max(0.0, dot(bump, halfDir)); // blinn-phong half4 albedo = tex2D(_MainTex, i.uv.xy) * _Color; // 采样常规纹理，计算漫反射系数 half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo.rgb; // 计算环境光 half3 diffuse = _LightColor0.rgb * albedo.rgb * NdotL; // 计算漫反射光 half3 specular = _LightColor0.rgb * _Specular.rgb * pow(NdotV, _SpecualrPower); // 计算高光 UNITY_LIGHT_ATTENUATION(atten, i, worldPos); // 计算光照衰减和阴影 return half4(ambient + (diffuse + specular) * atten, 1.0); } ENDCG } Pass { Tags {\u0026#34;LightMode\u0026#34;=\u0026#34;ForwardAdd\u0026#34;} CGPROGRAM // addtional Pass 逐像素渲染 #pragma multi_compile_fwdadd #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; #include \u0026#34;AutoLight.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; sampler2D _BumpTex; float4 _BumpTex_ST; float4 _Color; float4 _Specular; float _SpecualrPower; struct appdata { float4 vertex : POSITION; float4 texcoord : TEXCOORD0; float4 tangent : TANGENT; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float4 uv : TEXCOORD0; float4 TtoW0 : TEXCOORD1; float4 TtoW1 : TEXCOORD2; float4 TtoW2 : TEXCOORD3; SHADOW_COORDS(4) // 声明一个光源的阴影映射纹理 }; v2f vert (appdata v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); o.uv.xy = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; // 应用 _MainTex 的缩放和平移 o.uv.zw = v.texcoord.xy * _BumpTex_ST.xy + _BumpTex_ST.zw; // 应用 _BumpTex 的缩放和平移 half3 worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; // 顶点位置转到世界空间 half3 worldNormal = UnityObjectToWorldNormal(v.normal); // 顶点法线转到世界空间 half3 worldTangent = UnityObjectToWorldDir(v.tangent.xyz); // 顶点切线转到世界空间 half3 worldBinormal = cross(worldNormal, worldTangent) * v.tangent.w; // 计算得到世界空间下的顶点副切线 o.TtoW0 = float4(worldTangent.x, worldBinormal.x, worldNormal.x, worldPos.x); o.TtoW1 = float4(worldTangent.y, worldBinormal.y, worldNormal.y, worldPos.y); o.TtoW2 = float4(worldTangent.z, worldBinormal.z, worldNormal.z, worldPos.z); TRANSFER_SHADOW(o); // 计算声明过的阴影映射纹理 return o; } fixed4 frag (v2f i) : SV_Target { half3 worldPos = half3(i.TtoW0.w, i.TtoW1.w, i.TtoW2.w); // 世界空间下的坐标 half3 worldLightDir = normalize(UnityWorldSpaceLightDir(worldPos)); // 世界空间下的光源方向 half3 worldViewDir = normalize(UnityWorldSpaceViewDir(worldPos)); // 世界空间下的视角方向 half3 halfDir = normalize(worldLightDir + worldViewDir); // 半角方向 half3 bump = UnpackNormal(tex2D(_BumpTex, i.uv.zw)); // 采样法线贴图 bump = normalize(half3(dot(i.TtoW0.xyz, bump), dot(i.TtoW1.xyz, bump), dot(i.TtoW2.xyz, bump)));// 法线坐标从切线空间转到世界空间 half NdotL = max(0.0, dot(bump, worldLightDir)); // lambert half NdotV = max(0.0, dot(bump, halfDir)); // blinn-phong half4 albedo = tex2D(_MainTex, i.uv.xy) * _Color; // 采样常规纹理，计算漫反射系数 half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo.rgb; // 计算环境光 half3 diffuse = _LightColor0.rgb * albedo.rgb * NdotL; // 计算漫反射光 half3 specular = _LightColor0.rgb * _Specular.rgb * pow(NdotV, _SpecualrPower); // 计算高光 UNITY_LIGHT_ATTENUATION(atten, i, worldPos); // 计算光照衰减和阴影 return half4((diffuse + specular) * atten, 1.0); } ENDCG } } Fallback \u0026#34;Diffuse\u0026#34; } 总结 该章主要讲述了前向渲染中，不同类型光源的处理方式，以及 Unity 如何计算光照阴影 (shadowmap 技术)，最后又探讨了一下透明物体的阴影计算。\n  shader 中有两个 Pass，Base Pass 逐像素计算最重要的平行光，以及所有逐顶点光源和 SH 光源；Additional Pass 根据权重逐像素计算其它类型的光源，每个光源执行一次 Pass。\n  计算光照衰减，常用的是纹理查找，也可直接用数学方法进行计算。\n#ifdef USING_DIRECTIONAL_LIGHT // 获取平行光的方向 fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); #else // 通过点光源（或聚光灯）的位置减去世界空间下顶点位置，获得光的方向 fixed3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz - i.worldPos.xyz); #endif // 判断当前光源是否是平行光 #ifdef USING_DIRECTIONAL_LIGHT // 平行光衰减值为1 fixed atten = 1.0; #else // 如果是点光源，光的衰减值计算 #if defined(POINT) float3 lightCoord = mul(unity_WorldToLight, float4(i.worldPos, 1)).xyz; fixed atten = tex2D(_LightTexture0, dot(lightCoord, lightCoord).rr).UNITY_ATTEN_CHANNEL; // 如果是聚光灯，光的衰减值计算 #elif defined(SPOT) float4 lightCoord = mul(unity_WorldToLight, float4(i.worldPos, 1)); fixed atten = (lightCoord.z \u0026gt; 0) * tex2D(_LightTexture0, lightCoord.xy / lightCoord.w + 0.5).w * tex2D(_LightTextureB0, dot(lightCoord, lightCoord).rr).UNITY_ATTEN_CHANNEL; #else fixed atten = 1.0; #endif #endif   ","date":"2021-06-01T20:21:12+08:00","image":"https://yantree.github.io/p/shader%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81-%E6%9B%B4%E5%8A%A0%E5%A4%8D%E6%9D%82%E7%9A%84%E5%85%89%E7%85%A7/topimg_hud5b4711730563b8f6fe841cfaa71f45d_221051_120x120_fill_box_smart1_2.png","permalink":"https://yantree.github.io/p/shader%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81-%E6%9B%B4%E5%8A%A0%E5%A4%8D%E6%9D%82%E7%9A%84%E5%85%89%E7%85%A7/","title":"Shader入门精要-更加复杂的光照"},{"content":"技术美术——透明效果 Unity 中通常使用两种方法来实现透明：透明度测试 (AlphaTest) 和 透明度混合 (AlphaBlend) 。\n 透明度测试：一个片元透明度不满足条件(小于某个阈值)，那么它对应的片元就会被舍弃。被舍弃的片元将不再进行任何处理，不如写入颜色信息到 color buffer；否则，按照普通的不透明物体的处理方式来处理它，即进行 深度测试、深度写入 等。透明度测试不需要关闭深度写入，它与其他不透明物体最大的不同就是它会根据透明度来舍弃一些片元，所以它产生的效果很极端，要么完全透明，要么完全不透明，不能实现半透明效果。 透明度混合：使用当前片元的透明度作为混合因子，与已经存储在 color buffer 中的颜色值进行混合，得到新的颜色。透明度混合需要 关闭深度写入，也就是当前物体的深度信息不会被记录，但是 深度测试是开启的，也就是说当使用透明度混合渲染一个片元时，还是会比较当前物体的深度值与 depth buffer 中的深度值，如果当前物体的深度值距离摄像机更远，那么就不再进行混合操作。这一点决定了，一个不透明物体出现在一个透明物体的前面，先渲染了不透明物体，它可以正常的遮挡住透明物体。归根结底，对于透明度混合，depth buffer 是只可读的。  渲染顺序 对于透明度混合技术，我们需要关闭深度写入，但是关闭深度写入，那我们就需要小心处理透明物体的渲染顺序。\n为什么需要关闭深度写入？\n如果不关闭深度写入，半透明物体表面背后的面本来可以透过表面看到背后的面，由于深度测试判断半透明物体表面距离摄像机更近，就会导致表面背后的面被剔除，也就无法透过表面看到背后的面。\n关闭深度写入会发生什么？\n假设我们要渲染两个物体，一个是半透明物体 A，一个是不透明物体 B，A 在 b 前面 ( A 离摄像机更近)\n 一，先渲染 B，再渲染 A。因为不透明物体开启了深度测试和深度写入，所以 B 的数据会写进深度缓存里，当我们渲染 A 的时候，先提取深度缓存中的数据，然后和 A 进行透明度混合，显示结果正确。 二，先渲染 A，再渲染 B。由于半透明物体关闭了深度吸入，A 的深度信息不会写入深度缓存里；当渲染 B 的时候，B 的深度信息直接覆盖写入深度缓存里。实际上 B 应该再 A 的后面，但是从视角来看，B 出现在了 A 前面，显示结果错误。  又假设两个物体都是半透明物体呢？假设我们有两个物体 A 和 B，A 在 B 的前面（离摄像机更近），并且两者都是半透明物体。\n 一，先渲染 B，再渲染 A，B 的颜色会被写入颜色缓冲（和深度缓冲很像，可以理解为当前颜色），这样 A 会正确的获得颜色缓冲中的 B 的颜色数据，然后正确的混合。 二，先渲染 A，再渲染 B，A 的数据会被先写入颜色缓冲，然后渲染 B 的时候，会提取 A 的数据，和 B 进行混合。这样最终结果看起来像是 B 在 A 的前面。这就是错误的结果。  引擎如何处理透明度混合技术  先渲染所有的不透明物体，并开启他们的深度写入和深度测试。 把半透明物体按他们距离摄像机的远近进行排序，然后按照从后往前的顺序渲染这些半透明物体，并开启他们的深度测试，但关闭深度写入。  引擎无法处理的问题  \n在上图中，由于3个物体相互重叠，我们不可能的得到一个正确的排序顺序。这时候，我们可以选择把物体拆分成两个部分，然后再进行正确的排序。\n但即便我们通过分割的方法解决了循环覆盖的问题，还是会有其他的情况来捣乱。\n \n一个物体的网格结构往往占据了空间中的某一块区域，也就是说，这个网格上每一个点的深度值可能都不一样，我们选择哪个深度值来作物整个物体的深度值和其他物体进行排序呢？不幸的是基于上图的情况，无论我们选择哪个点，排序的结果都是A物体在B物体前面。这种问题解决方案通常也是分割网格。\nUnityShader 渲染顺序 Unity 通过一组 Queue 标签来决定模型属于哪个 渲染队列，队列由整数索引表示，值越小越先渲染。\n   名称 队列索引号 描述     Background 1000 最开始渲染的队列，通常使用该队列来渲染那些需要绘制再背景上的物体   Geometry 2000 默认的渲染队列，大多数物体使用这个队列。不透明物体使用该队列   AlphaTest 2450 需要透明度测试的物体使用这个队列。   Transparent 3000 在所有 Geometry 和 AlphaTest 物体渲染后，再按从后往前的顺序进行渲染，任何使用了透明度混合（例如关闭深度写入的 Shader）的物体都应该使用该队列   Overlay 4000 用于实现一些叠加效果，任何需要再最后渲染的物体都应该使用该队列    如果我们要控制物体的渲染顺序，需要在着色器中指出。\nSubShader{ Tags{\u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34;} // Tansparent 渲染队列 Pass{ // 变比深度写入 ZWrite Off //...other code } } 透明度测试 透明度测试简单粗暴，片元透明度不满足条件则被舍弃，且被舍弃的片元不会参与后面的计算，深度写入，颜色写入。再片元着色器中使用 clip 函数来进行透明度测试，定义如下。\n 函数：void clip(float4 x); void clip(float3 x); void clip(float2 x); void clip(float x); 参数：裁剪时使用的标量或矢量条件； 描述：如果给定参数任何一个分量是负数，就会舍弃当前像素的输出颜色。  Shader \u0026#34;Unlit/AlphaTestOff\u0026#34; { Properties { _MainTex (\u0026#34;Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _AlphaClip(\u0026#34;AlphaClip\u0026#34;,Range(0.0, 1)) = 0.5 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;TransparentCutout\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;AlphaTest\u0026#34; \u0026#34;IgnoreProjector\u0026#34;=\u0026#34;true\u0026#34; } // 关闭剔除效果 Cull Off Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; float _AlphaClip; struct appdata { float4 vertex : POSITION; float2 texcoord : TEXCOORD0; float3 normal : NORMAL; }; struct v2f { float4 posWorld : SV_POSITION; float2 uv : TEXCOORD0; float3 worldNormal : TEXCOORD1; float3 worldPos : TEXCOORD2; }; v2f vert (appdata v) { v2f o; o.uv = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; o.posWorld = UnityObjectToClipPos(v.vertex); o.worldNormal = UnityObjectToWorldNormal(v.normal); o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; return o; } fixed4 frag (v2f i) : SV_Target { half3 worldNormal = normalize(i.worldNormal); half3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); half3 halfDir = normalize(worldLightDir + worldViewDir); half NdotL = max(0.0, dot(worldNormal, worldLightDir)); half NdotV = max(0.0, dot(worldNormal, halfDir)); half4 albedo = tex2D(_MainTex, i.uv); half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo.rgb; half3 diffuse = _LightColor0.rgb * albedo.rgb * (NdotL * 0.5 +0.5); clip(albedo.a - _AlphaClip); // 简单粗暴裁剪掉片元，被裁减片元不再参与之后的计算 return half4(ambient + diffuse, 1.0); } ENDCG } } } 透明度混合 可以实现真正的半透明效果。使用当前片元的透明度作为混合因子，与已经存储在颜色缓存中的颜色值进行混合，得到新的颜色。但是，需要关闭深度写入，要非常小心物体的渲染顺序。为了进行混合，还需要使用 Unity 提供的混合命令 - Blend，Blend 是 Unity 提供的设置混合模式的命令。\n   语义 描述     Blend Off 关闭混合   Blend SrcFactor DstFactor 开启混合，设置混合因子。源颜色 (片元颜色) 乘以 SrcFactor，而目标颜色 (color buffer的颜色)乘以 DstFactor，然后把两者相加后再存入颜色缓冲中   Blend SrcFactor DstFactor, SrcFactorA DstFactorA 和上面几乎一样，只是使用不同的因子来混合透明通道   BlendOp BlendOperation 并非把源颜色与目标颜色简单相加后混合，而是使用 BlendOperation 对他们进行其他操作    这里使用第二种语义，即 Blend SrcFactor DsFactor 来进行混合。我们需要把源颜色的混合因子 SrcFactor 设置为 SrcAlpha，而目标颜色的混合因子 DstFactor 设为 OneMinusSrcAlpha 。这样意味着经过混合后新的颜色是：\n DstColornew = SrcAlpha * SrcColor + ( 1 - SrcAlpha ) * DstColorold\n 开启深度写入的半透明效果 关闭深度写入会带来各种问题，下图给出了由于排序错误而产生的错误的透明效果，这是由于我们关闭了深度写入造成的。\n \n为了解决这一问题，我们给出了一种解决办法，使用两个 Pass 来渲染模型：\n 第一个 Pass 开启深度写入，但不输出任何颜色信息，它的目的仅仅是为了把该模型的深度值写入深度缓冲区中； 第二个 Pass 进行正常的透明度混合，由于上一个 Pass 已经得到了逐像素的正确的深度信息，该 Pass 就可以按照像素级别的深度排序结果进行透明渲染。  这样做的缺陷在于，多使用一个 Pass 会对性能造成一定的影响。\n \n我们使用同上面透明度混合同样的代码，仅仅增加一个 Pass 用于深度写入。在该 Pass 的第一行，我们开起了深度写入；第二行我们使用了一个新的渲染命令—— ColorMask，在 ShaderLab 中，ColorMask 用于设置颜色通道的掩码。语义如下：\n ColorMask RGB | A | 0 | 其他任何R、G、B、A的组合\n 当ColorMask设置为0时，意味着该Pass不写入仍和颜色通道，即不会输出任何颜色。这正是我们所需要的——该Pass只需要写入深度缓存即可。这是 单面渲染 详细代码：\nShader \u0026#34;Unlit/AlphaBlendOneSlide\u0026#34; { Properties { _MainTex (\u0026#34;MainTex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Color(\u0026#34;Color\u0026#34;, Color) = (1,1,1,1) _AlphaBlend(\u0026#34;AlphaBlend\u0026#34;, Range(0.0, 1)) = 0.5 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;IngoreProjector\u0026#34;=\u0026#34;True\u0026#34; } Pass { // 开启深度写入 ZWrite On // 不写入任何颜色信息到 color buffer ColorMask 0 } Pass { Tags {\u0026#34;LightModel\u0026#34;=\u0026#34;ForwardBase\u0026#34;} // 关闭深度写入，也就是不将深度值存入深度buffer中 ZWrite Off // 混合方式 Blend SrcAlpha OneMinusSrcAlpha CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; float _AlphaBlend; float4 _Color; struct appdata { float4 vertex : POSITION; float3 normal : NORMAL; float2 texcoord : TEXCOORD0; }; struct v2f { float4 posWorld : SV_POSITION; float2 uv : TEXCOORD0; float3 worldNormal : TEXCOORD1; float3 worldPos : TEXCOORD2; }; v2f vert (appdata v) { v2f o; o.posWorld = UnityObjectToClipPos(v.vertex); // 顶点从模型空间转到裁剪空间 o.uv = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; // _MainTex 的缩放和平移 o.worldNormal = UnityObjectToWorldNormal(v.normal); // 法线从模型空间转到世界空间 o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; // 顶点位置从模型空间转到世界空间 return o; } fixed4 frag (v2f i) : SV_Target { half3 worldNormal = normalize(i.worldNormal); // 归一化世界法线 half3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); // 归一化主光源方向 half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); // 归一化视角方向 half3 halfDir = normalize(worldLightDir + worldViewDir); // 半角方向 half NdotL = max(0.0, dot(worldNormal, worldLightDir)); // lambert half NdotV = max(0.0, dot(worldNormal, halfDir)); half4 texColor = tex2D(_MainTex, i.uv); // 从纹理读取漫反射系数 half3 albedo = texColor.rgb * _Color.rgb; half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; // 环境光 half3 diffuse = _LightColor0.rgb * albedo * (NdotL * 0.5 + 0.5);// half lambert return half4(ambient + diffuse, texColor.a * _AlphaBlend); } ENDCG } } } 双面渲染\nShader \u0026#34;Unlit/AlphaBlendTwoSlide\u0026#34; { Properties { _MainTex (\u0026#34;MainTex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Color(\u0026#34;Color\u0026#34;, Color) = (1,1,1,1) _Alpha(\u0026#34;Alpha\u0026#34;, Range(0.0, 1)) = 0.5 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;IgnoreProjector\u0026#34;=\u0026#34;True\u0026#34; } Pass { Tags {\u0026#34;LightModel\u0026#34;=\u0026#34;ForwardBase\u0026#34;} Cull Front // 剔除正面 ZWrite off // 关闭深度写入 Blend SrcAlpha OneMinusSrcAlpha // 混合方式 CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; float4 _Color; float _Alpha; struct appdata { float4 vertex : POSITION; float2 texcoord : TEXCOORD0; float3 normal : NORMAL; }; struct v2f { float4 posWorld : SV_POSITION; float2 uv : TEXCOORD0; float3 worldNormal : TEXCOORD1; float3 worldPos : TEXCOORD2; }; v2f vert (appdata v) { v2f o; o.posWorld = UnityObjectToClipPos(v.vertex); o.uv = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; // _MainTex 的缩放和平移 o.worldNormal = UnityObjectToWorldNormal(v.normal); // 法线从模型空间转世界空间 o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; // 顶点位置从模型空间转世界空间 return o; } fixed4 frag (v2f i) : SV_Target { half3 worldNormal = normalize(i.worldNormal); // 归一化法线方向 half3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); // 归一化主光源方向 half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); // 归一化视角方向 half3 halfDir = normalize(worldLightDir + worldViewDir); // 半角方向 half3 NdotL = max(0.0, dot(worldNormal, worldLightDir)); half4 texColor = tex2D(_MainTex, i.uv); half3 albedo = texColor.rgb * _Color.rgb; half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; half3 diffuse = _LightColor0.rgb * albedo * (NdotL*0.5 +0.5); return half4(ambient + diffuse, texColor.a * _Alpha); } ENDCG } Pass { Tags {\u0026#34;LightModel\u0026#34;=\u0026#34;ForwardBase\u0026#34;} Cull Back // 剔除背面 ZWrite off // 关闭深度写入 Blend SrcAlpha OneMinusSrcAlpha CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; float4 _Color; float _Alpha; struct appdata { float4 vertex : POSITION; float2 texcoord : TEXCOORD0; float3 normal : NORMAL; }; struct v2f { float4 posWorld : SV_POSITION; float2 uv : TEXCOORD0; float3 worldNormal : TEXCOORD1; float3 worldPos : TEXCOORD2; }; v2f vert (appdata v) { v2f o; o.posWorld = UnityObjectToClipPos(v.vertex); o.uv = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; // _MainTex 的缩放和平移 o.worldNormal = UnityObjectToWorldNormal(v.normal); // 法线从模型空间转世界空间 o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; // 顶点位置从模型空间转世界空间 return o; } fixed4 frag (v2f i) : SV_Target { half3 worldNormal = normalize(i.worldNormal); // 归一化法线方向 half3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); // 归一化主光源方向 half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); // 归一化视角方向 half3 halfDir = normalize(worldLightDir + worldViewDir); // 半角方向 half3 NdotL = max(0.0, dot(worldNormal, worldLightDir)); half4 texColor = tex2D(_MainTex, i.uv); half3 albedo = texColor.rgb * _Color.rgb; half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; half3 diffuse = _LightColor0.rgb * albedo * (NdotL*0.5 +0.5); return half4(ambient + diffuse, texColor.a * _Alpha); } ENDCG } } } 总结 关于半透明物体的渲染解决方法，多添加一个 Pass；\n单面渲染，第一个 Pass 开启深度写入 ZWrite On 且不要写入任何信息(颜色、透明度) ColorMask 0；第二个 Pass 关闭深度写入 ZWrite Off ，设置好混合方式 Blend SrcAlpha OneMinusSrcAlpha 。\n双面渲染，两个 Pass 同时关闭深度写入 ZWrite On，设置好混合方式 Blend SraAlpha OneMinusSrcAlpha，第一个 Pass 剔除正面 Cull Front，第二个 Pass 剔除背面 Cull Back。\n调整透明度，从常规纹理那儿读取纹理的 Alpha 值\nhalf4 texColor = tex2D(_MainTex, i.uv); // 纹理采样  half3 albedo = texColor.rgb * _Color.rgb; half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; // 环境光 half3 diffuse = _LightColor0.rgb * albedo * (NdotL * 0.5 + 0.5);// half lambert return half4(ambient + diffuse, texColor.a * _AlphaBlend); ","date":"2021-06-01T14:24:53+08:00","image":"https://yantree.github.io/p/shader%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81-%E9%80%8F%E6%98%8E%E6%95%88%E6%9E%9C/topimg_hucec924f8a8d5e17115b60defab86f43b_300097_120x120_fill_box_smart1_2.png","permalink":"https://yantree.github.io/p/shader%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81-%E9%80%8F%E6%98%8E%E6%95%88%E6%9E%9C/","title":"Shader入门精要-透明效果"},{"content":"技术美术——基础纹理 常规纹理 纹理的一种使用方式就是作为常规纹理，可以理解为一张照片。在这里纹理的作用是代替 物体的漫反射系数 ，这里我们会再之前的 Blinn-Phong 高光反射 shader 的基础上实现一个基础的纹理 shader。这里可以知道常规纹理会参与 环境光 ，物体 漫反射 的计算。在基本光照模型中，由于没有相关材质贴图，所以在计算公式中\n C = (c * m) * max(0 , n * l )\n m 的值取为 1，不会影响到漫反射光的计算。当 m 值存在时，也就是有常规纹理的时候，光照计算中的 m 就要从常规纹理图中读取，读取方法见下面 albedo 的计算方法。\nShader \u0026#34;Unlit/MainTex0\u0026#34; { Properties { _MainTex (\u0026#34;Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Color(\u0026#34;Color\u0026#34;, Color) = (1,1,1,1) _SpecularCol(\u0026#34;Specular Col\u0026#34;, Color) = (1,1,1,1) _SpecularStrength(\u0026#34;Specular Strength\u0026#34;, Range(8.0, 256))=10 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;LightMode\u0026#34;=\u0026#34;ForwardBase\u0026#34; } LOD 100 Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; half4 _Color; half4 _SpecularCol; float _SpecularStrength; struct appdata { float4 vertex : POSITION; float3 normal : NORMAL; float2 texcoord : TEXCOORD0; }; struct v2f { float4 posWorld : SV_POSITION; float2 uv : TEXCOORD1; float3 worldNormal : NORMAL; float3 worldPos : TEXCOORD2; }; v2f vert (appdata v) { v2f o; o.posWorld = UnityObjectToClipPos(v.vertex); // 顶点位置从模型空间转到裁剪空间 o.worldNormal = UnityObjectToWorldDir(v.normal); // 顶点法线从模型空间转到世界空间 o.worldPos = UnityObjectToWorldDir(v.vertex); // 顶点从模型空间转到世界空间 o.uv = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; // 使用纹理属性的_ST对顶点纹理坐标进行变换 return o; } fixed4 frag (v2f i) : SV_Target { half3 worldNormal = normalize(i.worldNormal); // 顶点法线方向 half3 worldLightDir = normalize(_WorldSpaceLightPos0.xyz); // 主光源放方向 half3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.worldPos); // 视角方向 half3 halfDir = normalize(viewDir + worldLightDir); // 半角方向 half3 NdotL = max(0.0, dot(worldNormal, worldLightDir)); // Lambert half3 NdotV = max(0.0, dot(worldNormal, halfDir)); // BlinnPhong half3 albedo = tex2D(_MainTex,i.uv).rgb * _Color.rgb; // 漫反射系数，从纹理采样漫反射颜色作为其值 half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; // 环境光 // 漫反射光 = 入射光线强度 * 材质的漫反射系数 * 取值为正数(表面法线方向 · 光源方向) half3 diffuse = _LightColor0.rgb * albedo * pow(NdotL * 0.5 + 0.5, 2.0); // half lambert half3 specular = _LightColor0.rgb * _SpecularCol.rgb * pow(NdotV, _SpecularStrength); // blinn phong return fixed4(diffuse + specular + ambient, 1); } ENDCG } } } 凹凸映射 凹凸映射的作用就是使用一张纹理来修改模型表面的法线，提供更多的细节效果。主要有两种方法：\n  高度映射\n一张高度纹理来模拟表面位移，然后得到一个修改后的法线值。\n  法线映射\n一张法线纹理来直接存储表面法线。\n  通常我们将高度映射和法线映射当成一种技术，但是两者的本质是有所区别的。\n法线纹理 法线纹理中存储的是表面的法线方向，由于发现分量的取值范围在 [-1, 1] 之间，而像素的分量范围在 [0, 1] ，因此两者的转换还需要做一个映射：\n pixel = (normal + 1) / 2\n 反映射后就能得到原先的法线方向\n normal = pixel * 2 - 1\n 法线的坐标空间   模型空间的法线纹理\n模型顶点的法线定义在模型空间中，一种直接的想法就是将修改后的模型空间中的表面法线存储在一张纹理中，这种纹理被称之为 模型空间的法线纹理。\n  切线空间的法线纹理\n实际应用中，我们常用模型顶点的 切线空间 存储法线。对于模型的每个顶点，都有属于自己的切线空间，模型的顶点就是切线空间的原点，顶点的法线作切线空间的 z 轴 (n)，顶点的切线方向作 x 轴 (t)，顶点的 y 轴可由 x、z 叉乘求得，也被称为 副切线 或 副法线 (b)，这就是 切线空间的法线纹理。\n  计算方式 由于法线纹理中存储的是切线空间下的方向，因此我们通常有两种选择：\n 在切线空间下进行光照计算，需要将光照方向、视角方向变换到切线空间下。 在世界空间下进行光照计算，需要把采样得到的法线方向变换到世界空间下，再和世界空间下的光照方向和视角方向进行计算。  效率上考虑，第一种优于第二种，因为整个空间的变换过程在顶点着色器中完成。\n通用性考虑，第二种优于第一种，因为有时需要在世界空间进行一次矩阵操作。\n切线空间计算 思路：1. 在顶点着色器里将模型空间下的光源方向、视角方向转到切线空间；2. 在片元着色器里直接对法线纹理进行采样，得到切线空间下的法线数值；3. 进行光照计算。\nShader \u0026#34;Unlit/MainBumpT\u0026#34; { Properties { _MainTex (\u0026#34;MainTex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _BumpTex(\u0026#34;BumpTex\u0026#34;,2D) = \u0026#34;white\u0026#34; {} _BumpStrength(\u0026#34;Bump Strength\u0026#34;, Float) = 1.0 _Color(\u0026#34;Color Tint\u0026#34;, Color) = (1,1,1,1) _Specular(\u0026#34;Specular Color\u0026#34;, Color) = (1,1,1,1) _SpecularStrength(\u0026#34;Specular Strength\u0026#34;, Range(8.0, 256)) = 20 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;LightModel\u0026#34;=\u0026#34;ForwardBase\u0026#34; } LOD 100 Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag //#include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; sampler2D _BumpTex; float4 _BumpTex_ST; half _BumpStrength; half4 _Color; half4 _Specular; half _SpecularStrength; struct appdata { float4 vertex : POSITION; float4 texcoord : TEXCOORD0; float4 tangent : TANGENT; float3 normal : NORMAL; }; struct v2f { float4 uv : TEXCOORD0; // flaot4 因为使用了两张纹理贴图，所以成 flaot4 了 float4 posWorld : SV_POSITION; float3 lightDir : TEXCOORD1; float3 viewDir : TEXCOORD2; }; v2f vert (appdata v) { v2f o; o.posWorld = UnityObjectToClipPos(v.vertex); // 顶点坐标从模型空间转到裁剪空间 o.uv.xy = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; // MainTex 的 Tiling 和 Offset o.uv.zw = v.texcoord.xy * _BumpTex_ST.xy + _BumpTex_ST.zw; // BumpTex 的 Tiling 和 offset float3 binnormal = cross(normalize(v.normal), normalize(v.tangent.xyz)) * v.tangent.w; // 得到模型空间下的副切线(.w 是取一个方向作为切线方向) float3x3 rotation = float3x3(v.tangent.xyz, binnormal, v.normal); // 一个切线空间 (顶点切线方向作为 x,叉乘值作为 y, 顶点法线作为 z) o.lightDir = mul(rotation, ObjSpaceLightDir(v.vertex)); // 模型空间下的光照方向转到切线空间 o.viewDir = mul(rotation, ObjSpaceViewDir(v.vertex)); // 模型空间下的视角方向转到切线空间 return o; } fixed4 frag (v2f i) : SV_Target { half3 tangentLightDir = normalize(i.lightDir); // 归一化光照方向 half3 tangentViewDir = normalize(i.viewDir); // 归一化视角方向 half3 halfDir = normalize(tangentLightDir + tangentViewDir); // 归一化半角方向 half4 packNormal = tex2D(_BumpTex,i.uv.zw); // 对法线贴图进行采样 half3 tangentNormal; tangentNormal=UnpackNormal(packNormal); tangentNormal.xy*=_BumpStrength; // 调整法线缩放，控制凹凸强度 tangentNormal.z = sqrt(1.0 - max(0.0, dot(tangentNormal.xy, tangentNormal.xy))); half3 NdotL = max(0.0, dot(tangentNormal, tangentLightDir)); // lambert half3 RdotL = max(0.0, dot(tangentNormal, halfDir)); // blinn phong half3 albedo = tex2D(_MainTex,i.uv) * _Color.rgb; // 漫反射系数(从贴图中读取) half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; // 环境光 half3 diffuse = _LightColor0.rgb * albedo * pow(NdotL*0.5 +0.5, 2.0); // half lambert half3 specular = _LightColor0.rgb * _Specular.rgb * pow(RdotL, _SpecularStrength); return half4(ambient + diffuse + specular, 1.0); } ENDCG } } } 世界空间计算 思路：1. 在顶点着色器里将模型空间下的顶点位置、顶点法线、顶点切线、顶点副切线转到世界空间，并创建法线从切线空间转世界空间的 矩阵，用于后面采样法线纹理将法线从切线空间转到世界空间；2. 计算世界空间下的光照方向、视角方向，对法线纹理进行采样，并且将其从切线空间转到世界空间；3. 进行光照计算。\nShader \u0026#34;Unlit/MainBumpW\u0026#34; { Properties { _MainTex (\u0026#34;MainTex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _BumpTex(\u0026#34;BumpTex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _BumpScale(\u0026#34;Bump Scale\u0026#34;, Float) = 1.0 _Color(\u0026#34;Color\u0026#34;, Color) = (1,1,1,1) _Specular(\u0026#34;Specular\u0026#34;, Color) = (1,1,1,1) _SpecularPower(\u0026#34;Specular Power\u0026#34;, Range(8.0, 256)) = 20 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;LightMode\u0026#34;= \u0026#34;ForwardBase\u0026#34;} Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; sampler2D _BumpTex; float4 _BumpTex_ST; float4 _Color; float4 _Specular; float _BumpScale; float _SpecularPower; struct appdata { float4 vertex : POSITION; float3 normal : NORMAL; float4 tangent : TANGENT; float4 texcoord : TEXCOORD0; }; struct v2f { float4 uv : TEXCOORD0; float4 posWorld : SV_POSITION; float4 TtoW0:TEXCOORD1; float4 TtoW1:TEXCOORD2; float4 TtoW2:TEXCOORD3; }; v2f vert (appdata v) { v2f o; o.posWorld = UnityObjectToClipPos(v.vertex); // 顶点坐标从模型空间转到裁剪空间 o.uv.xy = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; // _MainTex 缩放平移属性 o.uv.zw = v.texcoord.xy * _BumpTex_ST.xy + _BumpTex_ST.zw; // _BumpTex 缩放平移属性 half3 worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; // 顶点位置从模型空间转到世界空间 half3 worldNormal = UnityObjectToWorldNormal(v.normal); // 顶点法线从模型空间转到世界空间 half3 worldTangent = UnityObjectToWorldDir(v.tangent.xyz); // 顶点切线从模型空间转到世界空间 half3 worldBinormal = cross(worldNormal, worldTangent) * v.tangent.w; // 计算世界空间下的副切线 o.TtoW0 = float4(worldTangent.x, worldBinormal.x, worldNormal.x, worldPos.x); o.TtoW1 = float4(worldTangent.y, worldBinormal.y, worldNormal.y, worldPos.y); o.TtoW2 = float4(worldTangent.z, worldBinormal.z, worldNormal.z, worldPos.z); return o; } fixed4 frag (v2f i) : SV_Target { half3 worldPos = half3(i.TtoW0.w, i.TtoW1.w, i.TtoW2.w); half3 lightDir = UnityWorldSpaceLightDir(worldPos); half3 viewDir = UnityWorldSpaceViewDir(worldPos); half3 halfDir = normalize(lightDir + viewDir); half3 bump = UnpackNormal(tex2D(_BumpTex, i.uv.zw)); // 从法线贴出中读取法线信息 bump.xy*=_BumpScale; // 缩放法线贴图 bump.z = sqrt(1.0 - dot(bump.xy, bump.xy)); bump = normalize(half3(dot(i.TtoW0.xyz, bump), dot(i.TtoW1.xyz, bump), dot(i.TtoW2.xyz, bump)));// 切线空间转世界空间 half NdotL = max(0.0, dot(bump, lightDir)); // lambert half NdotV = max(0.0, dot(bump, halfDir)); // blinn phong half3 albedo = tex2D(_MainTex, i.uv).rgb * _Color.rgb; // 读取漫反射系数 half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; // 环境光 half3 diffuse = _LightColor0.rgb * albedo * pow(NdotL * 0.5 + 0.5, 2.0); half3 specular = _LightColor0.rgb * _Specular.rgb * pow(NdotV, _SpecularPower); return half4(diffuse + specular + ambient, 1.0); } ENDCG } } } 渐变纹理 如常规纹理一样，我们在渲染中使用纹理是为了定义一个物体的颜色。纹理其实可以用于存储任何表面属性，一种常见的作法就是使用纹理渐变来控制漫反射光照结果。其核心部分是纹理采样计算部分，使用了 Half Lambert 作为采样UV。\n 注意将纹理的 Wrap mode 设为 Clamp\n  渐变纹理 \nShader \u0026#34;Unlit/gradient\u0026#34; { Properties { _RampTex (\u0026#34;Ramp Texture\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Color(\u0026#34;Color Tint\u0026#34;, Color) = (1,1,1,1) _Specular(\u0026#34;Specular Color\u0026#34;, Color) = (1,1,1,1) _SpecularPower(\u0026#34;Specular Power\u0026#34;, Range(8.0, 256)) = 20 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;LightModel\u0026#34;=\u0026#34;ForwardBase\u0026#34; } Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; sampler2D _RampTex; float4 _RampTex_ST; float4 _Color; float4 _Specular; Float _SpecularPower; struct appdata { float4 vertex : POSITION; float3 normal : NORMAL; float2 texcoord : TEXCOORD0; }; struct v2f { float4 posWorld : SV_POSITION; float3 worldNormal : TEXCOORD1; float3 worldPos : TEXCOORD2; float2 uv : TEXCOORD0; }; v2f vert (appdata v) { v2f o; o.posWorld = UnityObjectToClipPos(v.vertex); // 顶点坐标从模型空间转到世界空间 o.worldNormal = UnityObjectToWorldNormal(v.normal); // 法线从模型空间转到世界空间 o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz; // 位置从模型空间转到世界空间 o.uv = v.texcoord.xy * _RampTex_ST.xy + _RampTex_ST.zw; // UV 的缩放和平移属性 return o; } fixed4 frag (v2f i) : SV_Target { half3 worldNormal = normalize(i.worldNormal); // 归一化法线方向 half3 worldLightDir = normalize(UnityWorldSpaceLightDir(i.worldPos)); // 归一化主光源方向 half3 worldViewDir = normalize(UnityWorldSpaceViewDir(i.worldPos)); // 归一化视角方向 half3 halfDir = normalize(worldLightDir + worldViewDir); // 归一化半角方向 half NdotL = max(0.0, dot(worldNormal, worldLightDir) * 0.5 + 0.5); // lambert half NdotV = max(0.0, dot(worldNormal, halfDir)); half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; // 环境光 half3 diffuseColor = tex2D(_RampTex, half2(NdotL, NdotL)).rgb * _Color.rgb; // 反射颜色 = 使用渐变纹理来采样半兰伯特光照生成的纹理 half3 diffuse = _LightColor0.rgb * diffuseColor; half3 specular = _LightColor0.rgb * _Specular.rgb * pow(NdotV, _SpecularPower); return half4(ambient + diffuse + specular, 1.0); } ENDCG } } FallBack \u0026#34;Specular\u0026#34; } 遮罩纹理 遮罩纹理可以让我们更加自由的控制模型表面的性质。遮罩纹理的使用一般是：通过采样得到遮罩纹理的纹素值，然后使用其中某个（或某几个）通道的值来与某种表面属性进行相乘，这样，当该通道的值为0时，可以保护表面不受该属性的影响。这里样例使用的高光的 mask ，并且是纹理的 r 通道。\n MainBump + MainBumpMask \nShader \u0026#34;Unlit/mask\u0026#34; { Properties { _MainTex (\u0026#34;MainTex\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _Color(\u0026#34;Color\u0026#34;, Color) = (1,1,1,1) _BumpTex(\u0026#34;BumpTex\u0026#34;, 2D) = \u0026#34;whire\u0026#34; {} _BumpScale(\u0026#34;BumptScale\u0026#34;,Float) = 1.0 _SpecularMask(\u0026#34;Specular Mask\u0026#34;, 2D) = \u0026#34;white\u0026#34; {} _SpecularScale(\u0026#34;Specular Scale\u0026#34;, Float) = 1.0 _Specular(\u0026#34;Specualr\u0026#34;, Color) = (1,1,1,1) _SpecularPower(\u0026#34;Specular Power\u0026#34;, Range(8.0, 256)) = 20 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; \u0026#34;LightModel\u0026#34;=\u0026#34;ForwardBase\u0026#34; } LOD 100 Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; sampler2D _MainTex; float4 _MainTex_ST; float4 _Color; sampler2D _BumpTex; float4 _BumpTex_ST; float _BumpScale; sampler2D _SpecularMask; float _SpecularScale; float4 _Specular; float _SpecularPower; struct appdata { float4 vertex : POSITION; float4 texcoord : TEXCOORD0; float3 normal : NORMAL; float4 tangent : TANGENT; }; struct v2f { float4 posWorld : SV_POSITION; float4 uv : TEXCOORD0; float3 lightDir : TEXCOORD1; float3 viewDir : TEXCOORD2; }; v2f vert (appdata v) { v2f o; o.posWorld = UnityObjectToClipPos(v.vertex); // 顶点坐标从模型空间转世界空间 o.uv.xy = v.texcoord.xy * _MainTex_ST.xy + _MainTex_ST.zw; // _MainTex 的缩放和平移 o.uv.zw = v.texcoord.xy * _BumpTex_ST.xy + _BumpTex_ST.zw; // _BumpTex 的缩放和平移 float3 binormal = cross(normalize(v.normal), normalize(v.tangent)) * v.tangent.w; // 副切线 float3x3 rotation = float3x3(v.tangent.xyz, binormal, v.normal); // 变换矩阵 o.lightDir = mul(rotation, ObjSpaceLightDir(v.vertex)).xyz; // 主光源方向从模型空间转到切线空间 o.viewDir = mul(rotation, ObjSpaceViewDir(v.vertex)).xyz; // 视角方向从模型空间转到切线空间 return o; } fixed4 frag (v2f i) : SV_Target { half3 tangentLightDir = normalize(i.lightDir); // 切线空间下主光源方向 half3 tangentViewDir = normalize(i.viewDir); // 切线空间下的视角方向 half3 halfDir = normalize(tangentLightDir + tangentViewDir); // 半角方向 half3 tangentNormal = UnpackNormal(tex2D(_BumpTex, i.uv.zw)); // 从法线贴图读取法线信息 tangentNormal.xy *= _BumpScale; // 缩放法线 tangentNormal.z = sqrt(1.0 - max(0.0, dot(tangentNormal.xy, tangentNormal.xy))); // 计算法线的 z 值 half NdotL = max(0.0, dot(tangentNormal, tangentLightDir)); // lambert half NdotV = max(0.0, dot(tangentNormal, halfDir)); // blinn-phong half3 albedo = tex2D(_MainTex, i.uv.xy).rgb * _Color.rgb; // 从材质贴图计算漫反射率 half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz * albedo; // 环境光 half3 diffuse = _LightColor0.rgb * albedo * (NdotL * 0.5 +0.5); // 计算漫反射光 half specularMask = tex2D(_SpecularMask, i.uv).r * _Specular.rgb; // 从高光贴图中读取高光反射值 half3 specular = _LightColor0.rgb * _Specular.rgb * specularMask * pow(NdotV, _SpecularPower); return half4(ambient + diffuse + specular, 1.0); } ENDCG } } } 拓展 ：Mask 的应用还有很大的空间，可以试着举例出来：UV 顶点动画。\n总结     漫反射系数 法线系数     无纹理 1 模型法线   常规纹理 tex2D(_MainTex,i.uv).rgb * _Color.rgb 模型法线   法线纹理 tex2D(_MainTex,i.uv) * _Color.rgb tex2D(_BumpTex,i.uv.zw)   渐变纹理 tex2D(_RampTex, half2(NdotL, NdotL)).rgb * _Color.rgb 模型法线   遮罩纹理 tex2D(_MainTex, i.uv.xy).rgb * _Color.rgb (tex2D(_BumpTex, i.uv.zw)    ","date":"2021-05-31T16:00:26+08:00","image":"https://yantree.github.io/p/shader%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81-%E5%9F%BA%E7%A1%80%E7%BA%B9%E7%90%86/topimg_hub743dc7ee0e867207b7b55e10230ffbf_426847_120x120_fill_box_smart1_2.png","permalink":"https://yantree.github.io/p/shader%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81-%E5%9F%BA%E7%A1%80%E7%BA%B9%E7%90%86/","title":"Shader入门精要-基础纹理"},{"content":"技术美术——基础光照模型 Lambert Lambert 光照模型的光照计算简单粗暴，是光源方向点成法线方向，其值在 [-1, 1] 之间，基本计算公式如下：\n C = (c * m) * max(0 , n * l )\n // 法线方向点乘光源方向 float NdotL = max(0.0,dot(normalDirection,lightDirection)); float LambertDiffuse = NdotL * SurfaceColor; float3 finalColor = LambertDiffuse * LightColor; Half Lambert Lambert 光照模型是一个简单方便的光照计算模型，但是，有一个问题存在。在光照无法照射的区域，模型外观变成了全黑，没有任何明暗的变化，使得模型背光区看起来像一个平面，失去了模型细节表现。可以通过添加环境光来得到非全黑的效果，但即使这样仍然无法解决背光面明暗一样的缺点。为了解决这个问题，有人在 Lambert 光照模型的基础上进行改良，这就是 Half Lambert 光照模型，通过乘以一个系数 a 再加上一个系数 b 的方法，将 Lambert 光照模型的值 [-1, 1] 重新映射至 [-a + b, a + b]，绝大多数情况下 a b 的值都取 0.5 ，这样 Half Lambert 光照模型的值 [0, 1]，在一定程度上改善了 Lambert 光照模型所带来的问题：\n//  float NdotL = max(0.0,dot(normalDirection,lightDirection) * 0.5 + 0.5); float HalfLambertDiffuse = NdotL * SurfaceColor; float3 finalColor = HalfLambertDiffuse * LightColor; Phong Lighting 高光计算部分，基本计算公式：\n Cspecular = (Clight * M specular)Max(0, V * r)\n 需要四个参数参与计算，入射光的颜色和强度 Clight，材质的高光反射系数 Mspecular，视线方向 v 以及反射方向 r。\n// 视线方向，摄像机坐标 - 顶点坐标得到视线方向，向量从顶点指向摄像机，就是视线方向  float3 viewDirection = normalize(_WorldSpaceCameraPos.xyz - i.posWorld.xyz); // 光线反射方向，有光源方向和顶点的法线方向计算得到 float3 lightReflectDirection = reflect( -lightDirection, normalDirection ); // Lambert 光照模型 float NdotL = max(0, dot( normalDirection, lightDirection )); float RdotV = max(0, dot( lightReflectDirection, viewDirection )); // 计算具体的高光，是一个指数函数，再乘以光照强度、高光颜色 float3 specularity = pow(RdotV, _SpecularGloss/4) *_SpecularPower *_SpecularColor.rgb ; float3 lightingModel = NdotL * diffuseColor + specularity; Blinn-Phong 高光计算部分，基本公式：\n Cspecular = (Clight * M specular)Max(0, n * h)\n 不同于 Phong 模型，这里不使用视线方向 v 和反射方向 r，取而代之的是引入新的矢量 h，通过视角方向 v 和光照方向 i 相加归一化处理后得到。\n// 视线方向，摄像机坐标 - 顶点坐标得到视线方向，向量从顶点指向摄像机，就是视线方向  float3 viewDirection = normalize(_WorldSpaceCameraPos.xyz - i.posWorld.xyz); // 半角方向，根据经验取得的模型，让视线方向+光源方向作为光反射的方向 float3 halfDirection = normalize(viewDirection+lightDirection); float NdotL = max(0, dot( normalDirection, lightDirection )); float NdotV = max(0, dot( normalDirection, halfDirection )); // 计算具体的高光，是一个指数函数，再乘以光照强度、高光颜色 float3 specularity = pow(NdotV ,_SpecularGloss)*_SpecularPower *_SpecularColor.rgb ; float3 lightingModel = NdotL * diffuseColor + specularity; 总结 在实际应用中，我们还是大量使用 Half Lambert 和 Blinn-Phong 光照模型，需要注意的是，这两个模型都是 经验 模型，我们从物理的角度看，这两个模型都是不正确的，但是，在实际的渲染中，这两个模型的结果更让人满意。最后给出适用的代码，可以根据需要自行注释掉关于光照部分计算的代码来观察不同光照模型之间的区别\n 四种不同光照模型的结果 \nShader \u0026#34;URP/HalfLambertBlinnPhong\u0026#34; { Properties { _Diffuse (\u0026#34;Diffuse\u0026#34;, Color) = (1,1,1,1) _Specular(\u0026#34;Specular\u0026#34;,Color) = (1,1,1,1) _Gloss(\u0026#34;Gloss\u0026#34;,Range(1.0,200))=10 } SubShader { Tags { \u0026#34;RenderPipeline\u0026#34;=\u0026#34;UniversalRenderPipeline\u0026#34; \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } Pass { Tags { \u0026#34;LightMode\u0026#34;=\u0026#34;UniversalForward\u0026#34;} HLSLPROGRAM // UnityCG.cginc 文件被代替，使用 HLSL 文件，这里引用文件 #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Core.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.universal/ShaderLibrary/Lighting.hlsl\u0026#34; #include \u0026#34;Packages/com.unity.render-pipelines.core/ShaderLibrary/SpaceTransforms.hlsl\u0026#34; #pragma vertex vert #pragma fragment frag CBUFFER_START(UnityPerMaterial) // 兼容 SRP 批处理，将所有材质属性声明在一个 UnityPerMaterial 的 CBUFFER 块中  float4 _Diffuse; float4 _Specular; float _Gloss; CBUFFER_END struct a2v { float4 vertex : POSITION; float3 normal : NORMAL; float2 lightmapUV : TEXCOORD1; }; struct v2f { float4 pos : SV_POSITION; float3 worldNormal : TEXCOORD0; float3 worldPos : TEXCOORD1; }; v2f vert(a2v v) { v2f o; ZERO_INITIALIZE(v2f, o); // 初始化变量 o.pos=TransformObjectToHClip(v.vertex.xyz); // 顶点坐标从模型空间转到裁剪空间 o.worldNormal = TransformObjectToWorldDir(v.normal); // 法线从模型空间转到世界空间 o.worldPos = TransformObjectToWorldDir(v.vertex.xyz); // 顶点从对象空间转到世界空间 return o; } half4 frag(v2f i):SV_Target { Light mainLight = GetMainLight(); // 获取主光源 half3 worldNormal = normalize(i.worldNormal); // 顶点法线方向归一化 half3 worldLightDir = normalize(TransformObjectToWorldDir(mainLight.direction)); // 主光源方向归一化 half3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.worldPos); // 计算视角方向并且归一化 half3 lightReflectDir = reflect( -worldLightDir, worldNormal ); // 计算 phong 模型中高光部分 half3 halfDir = normalize(worldLightDir + viewDir); // 计算半角方向且归一化 half3 NdotL = max(0.0, dot(worldNormal, worldLightDir)); // Lambert half3 RdotV = max(0.0, dot(worldNormal, lightReflectDir)); // Phong half3 NdotV = max(0.0, dot(worldNormal, halfDir)); // Blinn-Phong /// lambert + Phong //half3 diffuse = mainLight.color.rgb * _Diffuse.rgb * NdotL; //half3 specular = mainLight.color.rgb * _Specular.rgb * pow(RdotV, _Gloss); /// Lambert + Blinn-Phong //half3 diffuse = mainLight.color.rgb * _Diffuse.rgb * NdotL; //half3 specular = mainLight.color.rgb * _Specular.rgb * pow(NdotV, _Gloss); /// Half Lambert + Phong //half3 diffuse = mainLight.color.rgb * _Diffuse.rgb * pow(NdotL * 0.5 + 0.5, 2.0); //half3 specular = mainLight.color.rgb * _Specular.rgb * pow(RdotV, _Gloss); /// Half Lambert + Blinn-Phong half3 diffuse = mainLight.color.rgb * _Diffuse.rgb * pow(NdotL * 0.5 + 0.5, 2.0); half3 specular = mainLight.color.rgb * _Specular.rgb * pow(NdotV, _Gloss); // 计算环境光 half3 ambient = _GlossyEnvironmentColor; half4 col= half4(ambient + diffuse + specular, 1.0); return col; } ENDHLSL } } FallBack \u0026#34;Packages/com.unity.render-pipelines.universal/FallbackError\u0026#34; } Shader \u0026#34;SRP/HalfLambertBlinnPhong\u0026#34; { Properties { _Diffuse(\u0026#34;Diffuse Color\u0026#34;, Color) = (1,1,1,1) _SpcularColor(\u0026#34;Spcular Color\u0026#34;, Color) = (1,1,1,1) _SpcularStrength(\u0026#34;Spcular Strenght\u0026#34;, Range(0.0, 100)) = 10 } SubShader { Tags { \u0026#34;RenderType\u0026#34;=\u0026#34;Opaque\u0026#34; } LOD 100 Pass { CGPROGRAM #pragma vertex vert #pragma fragment frag #include \u0026#34;UnityCG.cginc\u0026#34; #include \u0026#34;Lighting.cginc\u0026#34; float4 _Diffuse; float4 _SpcularColor; float _SpcularStrength; struct appdata { float4 vertex : POSITION; float3 normal : NORMAL; }; struct v2f { float4 pos : SV_POSITION; float3 worldNormal : NORMAL; float3 worldPos : TEXCOORD0; }; v2f vert (appdata v) { v2f o; o.pos = UnityObjectToClipPos(v.vertex); // 顶点坐标从模型空间转到裁剪空间 o.worldNormal = UnityObjectToWorldDir(v.normal); // 将法线从模型空间转到世界空间 o.worldPos = UnityObjectToWorldDir(v.vertex); // 将顶点从模型空间转到世界空间 return o; } fixed4 frag (v2f i) : SV_Target { half3 ambient = UNITY_LIGHTMODEL_AMBIENT.xyz; // 获取环境光 half3 worldLightDir=normalize(_WorldSpaceLightPos0.xyz); // 主光源的方向 half3 worldNormal = normalize(i.worldNormal); // 法线方向 half3 viewDir = normalize(_WorldSpaceCameraPos.xyz - i.worldPos); // 视角方向 half3 lightReflectDir = reflect(-worldLightDir, worldNormal); // 主光源光线反射方向 half3 halfDir = normalize(viewDir + worldLightDir); half3 NdotL = max(0.0, dot(worldNormal, worldLightDir)); // lambert half3 RdotV = max(0.0, dot(worldNormal, lightReflectDir)); // phong half3 NdotV = max(0.0, dot(worldNormal, halfDir)); // blinn phong // lambert + phong //half3 diffuse = _LightColor0.rgb * _Diffuse * NdotL; //half3 specular = _LightColor0.rgb * _SpcularColor.rgb * pow(RdotV, _SpcularStrength); // Lambert + blinn phong //half3 diffuse = _LightColor0.rgb * _Diffuse * NdotL; //half3 specular = _LightColor0.rgb * _SpcularColor.rgb * pow(NdotV, _SpcularStrength); // half lambert + phong //half3 diffuse = _LightColor0.rgb * _Diffuse * pow(NdotL * 0.5 + 0.5, 2.0); //half3 specular = _LightColor0.rgb * _SpcularColor.rgb * pow(RdotV, _SpcularStrength); // half lambert + blinn phong half3 diffuse = _LightColor0.rgb * _Diffuse * pow(NdotL * 0.5 + 0.5, 2.0); half3 specular = _LightColor0.rgb * _SpcularColor.rgb * pow(NdotV, _SpcularStrength); return half4(diffuse + specular + ambient, 1); } ENDCG } } } 相关链接 Lighting Models In Unity - Jordan Stevens \n","date":"2021-05-28T22:34:22+08:00","image":"https://yantree.github.io/p/shader%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81-%E7%AE%80%E5%8D%95%E5%85%89%E7%85%A7%E6%A8%A1%E5%9E%8B/topimg_hubb21fb6872520e85a0401e3049fab639_84088_120x120_fill_q75_h2_box_smart1.webp","permalink":"https://yantree.github.io/p/shader%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81-%E7%AE%80%E5%8D%95%E5%85%89%E7%85%A7%E6%A8%A1%E5%9E%8B/","title":"Shader入门精要-简单光照模型"},{"content":"技术美术——渲染流水线 应用阶段(CPU)   将场景数据加载到显存\n  设置渲染状态\n这一步可配置的有 渲染状态，类似如下设置\nTags { \u0026#34;RenderType\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;Queue\u0026#34;=\u0026#34;Transparent\u0026#34; \u0026#34;IngoreProjector\u0026#34;=\u0026#34;True\u0026#34; Cull Front // 剔除正面 ZWrite off // 关闭深度写入 Blend SrcAlpha OneMinusSrcAlpha // 混合方式   调用 DrawCall\n  输出： 几何体数据（顶点坐标、法向量、 纹理坐标、纹理等）通过数据总线传送到图形硬件（时间瓶颈）。\n几何阶段(GPU) 输入： 上个阶段的输出。\n  顶点着色器\n根据应用阶段传进来的顶点位置做坐标变换（MVP变换矩阵）和逐顶点光照。\n  曲面细分着色器、几何着色器\n  裁剪、屏幕映射\n用透视变换矩阵把顶点从视锥体变换到裁剪空间的 CVV 中，这一步叫 投影 ； CVV 中进行图元裁剪，这一步叫 裁剪；将前述过程得到的坐标映射到 屏幕空间坐标系 上，这一步叫屏幕映射。\n  顶点着色器\n在顶点着色器中，我们经常要进行的操作是坐标空间的转换，根据光照计算的需要，我们可能会将顶点位置、法线、切线转到切线空间或者世界空间。\n结果： 得到经过变换与投影之后的顶点坐标、颜色、以及纹理坐标。\n 顶点着色器中计算光照叫逐顶点光照，还有一种实在片元着色器中计算光照叫逐像素光照。\n 光栅化阶段(GPU) 输入： 上个阶段的输出\n  三角形设置、遍历\n根据上一步输出的三角形顶点位置，计算三角形网格表示数据的过程叫做三角形设置；检查每个像素是否被一个三角网格覆盖，若被覆盖，则生成一个片元，且根据三个顶点的顶点信息对整个覆盖区域的像素进行插值，这个过程叫做三角形遍历。\n  片元着色器\n在这里最重要的技术之一就是纹理采样，通常会在顶点着色阶段输出每个顶点对应的纹理坐标，然后经过光栅化阶段对三角网格的三个顶点对应的纹理坐标进行插值后，就可以得到覆盖的片元的纹理坐标了。\n  逐片元操作\n决定每个片元的可见性，例如深度测试、模板测试等；\n混合操作，通过测试的片元需要把这个片元的颜色值和已经存储在颜色缓冲区中的颜色进行混合。\n  结果： 为像素（Pixel）正确配色，以便绘制 完整图像，该阶段进行的都是单个像素的操作，每个像素的信息存储在颜色缓冲 器（color buffer 或者 frame buffer）中。\n","date":"2021-05-23T22:20:04+08:00","image":"https://yantree.github.io/p/shader%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81-%E6%B8%B2%E6%9F%93%E6%B5%81%E6%B0%B4%E7%BA%BF/topimg_hu00ad781032300df6911004af6f547e11_134852_120x120_fill_box_smart1_2.png","permalink":"https://yantree.github.io/p/shader%E5%85%A5%E9%97%A8%E7%B2%BE%E8%A6%81-%E6%B8%B2%E6%9F%93%E6%B5%81%E6%B0%B4%E7%BA%BF/","title":"Shader入门精要-渲染流水线"},{"content":"技美——函数方法解释 [TextureName]_ST\n举例来说一个纹理贴图：_Main_ST ，其中 ST 代表的是纹理的缩放属性 (Scale) 和平移 (Translation) 的缩写。_Main_ST.xy 存储缩放值，_Main_ST.zw 存储偏移值。不知道什么是纹理的缩放和偏移，建议补一下计算机图形学，里面有详细的介绍。\nPOSITION vs SV_POSITION\nPOSITION 被用作 vertex shader 的输入，SV_POSITION 被用作 vertex shader 的输出，fragment shader 的输入。SV 是 Systems Value 的简写，SV_POSITION 用来标识经过 vertex shader 变换后的顶点坐标。\nUnityObjectToClipPos\n在unity5.6 以前是 UNITY_MATRIX_MVP，用法：\nunity 5.6 前：o.vertex = mul(UNITY_MATRIX_MVP,v.vertex);\nunity 5.6 后：o.vertex = UnityObjectToClipPos(v.vertex);\n到这里就因该明白了，UnityObjectToClipPos 是一个 MVP 变换方法。\nTRANSFORM_TEX\n将模型顶点的UV与Tiling、Offset两个变量进行运算，计算出实际显示用的顶点UV，在UnityCG.cginc中的定义如下：\n// Transforms 2D UV by scale/bias property #define TRANSFORM_TEX(tex,name) (tex.xy * name##_ST.xy + name##_ST.zw) o.uv = TRANSFORM_TEX (v.texcoord, _MainTex);\nv 是 appdata_base 类型，v.texcoord 就是模型顶点的 uv 数据。\n_MainTex 是使用的图片；\nname##_ST 实际上就是 _MainTex_ST； name## ST.xy 就是 Tiling 的 xy 值；\nname##_ST.zw 就是 Offset 的 xy 值。\nTex2D(_MainTex, IN.uv_MainTex)\nhalf4 c = tex2D (_MainTex, IN.uv_MainTex);\ntex2D(sampler2D tex, float2 s)函数，这是CG程序中用来在一张贴图中对一个点进行采样的方法，返回一个float4。这里对 _MainTex在输入点上进行了采样，并将其颜色的rbg值赋予了输出的像素颜色，将a值赋予透明度。于是，着色器就明白了应当怎样工作：即找到贴图上 对应的uv点，直接使用颜色信息来进行着色\nScene Depth / Screen Position\nScene Depth 是除开透明模型之外的深度，Screen Position是所有模型的深度。这里如果用视线空间的 Scene Depth 减去 Screen Position(Raw Model) 的 alpha，就可以得到透明模型与不透明模型相交的边缘。\nPosterize Node\n使用该节点会生成的代码：\nvoid Unity_Posterize_float4(float4 In, float4 Steps, out float4 Out) { Out = floor(In / (1 / Steps)) * (1 / Steps); } 将连续的值以 ( 1 / Steps ) 为单位向下取整，产生以 ( 1 / Steps ) 为单位标量的值。(说人话：Steps 值越大，新生成的值越接近原来的值；Steps 值越小，新生成的值越偏离原来的值，最终结果可能只剩下两个值。举例来说：一条函数曲线在 Sterps 越小的情况下越接近一条直线。)\nEllipse Node(Fragment)\n使用该节点会生成的代码：\nvoid Unity_Ellipse_float(float2 UV, float Width, float Height, out float4 Out) { float d = length((UV * 2 - 1) / float2(Width, Height)); Out = saturate((1 - d) / fwidth(d)); } 根据输入 UV 以输入宽度和高度指定的大小生成椭圆形状。\nTwirl Node\n使用该节点会生成的代码：\nvoid Unity_Twirl_float(float2 UV, float2 Center, float Strength, float2 Offset, out float2 Out) { float2 delta = UV - Center; float angle = Strength * length(delta); float x = cos(angle) * delta.x - sin(angle) * delta.y; float y = sin(angle) * delta.x + cos(angle) * delta.y; Out = float2(x + Center.x + Offset.x, y + Center.y + Offset.y); } 将类似于黑洞的旋转扭曲效果应用于输入UV的值。 变形效果的中心参考点由输入Center定义，效果的整体强度由输入Strength的值定义。 输入偏移量可用于偏移结果的各个通道。\n","date":"2021-05-22T20:09:54+08:00","permalink":"https://yantree.github.io/p/shadercode%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0%E8%A7%A3%E9%87%8A/","title":"ShaderCode常用函数解释"},{"content":"Visual Studio \u0026amp; OpenGL 这里采用的方案与 LearnOpenGL 一样，使用 glfw 和 glad。❗ 个人使用 win10 环境。\nTable of Contents\n Visual Studio \u0026amp; OpenGL  1. 配置 Visual Studio 2. 构建开发模块 3. visual Studio 编译 GLFW 4. 下载 GLAD 5. 设置项目 6. 测试用例    1. 配置 Visual Studio 安装基本的开发包，有两个必装的包。这里我使用了英文，在语言包里，我还选择了 English 安装包。\n 安装模块 \n2. 构建开发模块 新建一个 OpenGL 文件夹\n  下载 CMake ，新电脑选择 x64 的安装包，安装 CMake。\n  下载 GLFW官网，点击下载 Source package，并解压至 OpenGL 文件夹。\n  使用 CMake 编译 GLFW。\n  当前文件目录如下：\n 项目目录 \nbuild 文件夹是我自己创建的，待会编译会用到。\n  双击 CMake-gui 程序，进行编译。\n先设置好对应文件夹，如下图所示：source code 是解压后的 glfw-3.3.2 文件目录，build binaries 是我们新建的 build 文件夹，对应关系如上图所示。\n Cmake build glfw \n  接下来点击下面的 Configure 按钮。\n Cmake 配置 \n会弹出下图右边的小弹窗，确定选择的是 Visual Studio 16 2019 平台，完成后，点击下面的 Finish 按钮。\n  真正开始编译\n上面点击完 Finish 后等一会会出现如下图左边的红色页面，此时我们点击左边的 Generate 开始编译。\n Cmake 编译 \n此时，CMake 已经完成了它的所有任务。\n    3. visual Studio 编译 GLFW 在 CMake 完成编译后，上图的右边部分，我们可以直接点击 Open Project 用 Visual Studio 打开 project ，或者双击 build 后里的 GLFW.sln 文件，打开 project 后，点击 build -\u0026gt; build solution， 完成后会在 build -\u0026gt; src -\u0026gt; Debug 里生成 glfw3.lib 文件，这是我们需要的。\n4. 下载 GLAD 点击 这里 打开 GLAD 在线服务，设置如下：\n 下载GLAD \n确保 Options 下面勾选上了 Generate a loader，最后点击页面最下面的 GENERATE，此时会打开另一个页面，我们点击下面的 glad.zip ，下载 glad。将其解压在 OpenGL 文件夹下，此时 OpenGL 文件下会多出 include、src 文件夹。\n5. 设置项目 这一步我们会重点关注两个文件夹，前面的 OpenGL 和接下来要创建的项目文件夹 Learn OpenGL 。\n打开 Visual Studio 新建一个空的 project，结果如图：\n 创建空项目 \n在 LearnOpenGL 项目里新建文件夹 OpenGL_Stuff，并创建子文件夹 includes，lib，当前 LearnOpenGL 项目目录结构\n 项目目录 \n当前的 OpenGL 目录如下：\n 项目目录 \n将 OpenGL -\u0026gt; build -\u0026gt; src -\u0026gt; Debug 里生成 glfw3.lib 文件复制到 LearnOpenGL -\u0026gt; OpenGL_Stuff -\u0026gt; lib 文件夹中，将 OpenGL -\u0026gt; glfw-3.3.2 -\u0026gt; include 里的 GLFW 文件夹复制到 LearnOpenGL -\u0026gt; OpenGL_Stuff -\u0026gt; includes 文件夹中，将 **OpenGL -\u0026gt; include ** 里的 glad，KHR 文件夹复制到 LearnOpenGL -\u0026gt; OpenGL_Stuff -\u0026gt; includes 文件夹中。完成后的 LearnOpenGL 项目目录结构：\n 项目目录 \n双击 LearnOpenGL.sln 打开项目，还要将 OpenGL -\u0026gt; src 里的 glad.c 文件添加进 Learn OpenGL 项目里。打开 Visual Studio 后，直接将 glad.c 文件拖拽至 Source Files 目录里。\n  项目目录 \n接下来就是对 Learn OpenGL 项目属性设置了，打开项目后，Project -\u0026gt; Properties 。\n左边点击 VC++ Directories\n  编辑右边的 General -\u0026gt; Include Directories，选择 includes 文件路径，点击一下右下角的 Apply 按钮；\n  编辑右边的 General -\u0026gt; Library Directories，选择 lib 文件路径，点击一下右下角的 Apply 按钮；\n Visual Studio 添加文件路径 \n  左边点击 Linker -\u0026gt; Input\n编辑右边的 Additional Dependencies，输入 opengl32.lib 按一下回车继续输入 glfw3.lib ，点击一下右下角的 Apply 按钮；\n Visual Studio 添加依赖 \n到这儿，整个配置环境就搭建好了。\n6. 测试用例 新建cpp文件并测试，下面的是测试代码\n#include \u0026lt;glad/glad.h\u0026gt;#include \u0026lt;GLFW/glfw3.h\u0026gt; #include \u0026lt;iostream\u0026gt; void framebuffer_size_callback(GLFWwindow* window, int width, int height); void processInput(GLFWwindow* window); // settings const unsigned int SCR_WIDTH = 800; const unsigned int SCR_HEIGHT = 600; int main() { // glfw: initialize and configure  // ------------------------------  glfwInit(); glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 3); glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 3); glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE); #ifdef __APPLE__  glfwWindowHint(GLFW_OPENGL_FORWARD_COMPAT, GL_TRUE); #endif  // glfw window creation  // --------------------  GLFWwindow* window = glfwCreateWindow(SCR_WIDTH, SCR_HEIGHT, \u0026#34;LearnOpenGL\u0026#34;, NULL, NULL); if (window == NULL) { std::cout \u0026lt;\u0026lt; \u0026#34;Failed to create GLFW window\u0026#34; \u0026lt;\u0026lt; std::endl; glfwTerminate(); return -1; } glfwMakeContextCurrent(window); glfwSetFramebufferSizeCallback(window, framebuffer_size_callback); // glad: load all OpenGL function pointers  // ---------------------------------------  if (!gladLoadGLLoader((GLADloadproc)glfwGetProcAddress)) { std::cout \u0026lt;\u0026lt; \u0026#34;Failed to initialize GLAD\u0026#34; \u0026lt;\u0026lt; std::endl; return -1; } // render loop  // -----------  while (!glfwWindowShouldClose(window)) { // input  // -----  processInput(window); // render  // ------  glClearColor(0.2f, 0.3f, 0.3f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); // glfw: swap buffers and poll IO events (keys pressed/released, mouse moved etc.)  // -------------------------------------------------------------------------------  glfwSwapBuffers(window); glfwPollEvents(); } // glfw: terminate, clearing all previously allocated GLFW resources.  // ------------------------------------------------------------------  glfwTerminate(); return 0; } // process all input: query GLFW whether relevant keys are pressed/released this frame and react accordingly // --------------------------------------------------------------------------------------------------------- void processInput(GLFWwindow* window) { if (glfwGetKey(window, GLFW_KEY_ESCAPE) == GLFW_PRESS) glfwSetWindowShouldClose(window, true); } // glfw: whenever the window size changed (by OS or user resize) this callback function executes // --------------------------------------------------------------------------------------------- void framebuffer_size_callback(GLFWwindow* window, int width, int height) { // make sure the viewport matches the new window dimensions; note that width and  // height will be significantly larger than specified on retina displays.  glViewport(0, 0, width, height); } 如果有报错的话，很可能是上一步的 Project -\u0026gt; Properties 的属性设置被复原了，此时查看一下对应的 Include Directories ，Library Directories，Additional Dependencies 的设置，如果被复原了，再重复一遍上一步的项目属性设置的操作。完成后，再编译上面代码就能看到一个窗口了。\n  OpenGL 简单窗口 \n总结 配置过程中可能会出现的问题在第六步的最下面有提到过，这里的问题产生的原因在于项目创建之后，没有创建一份文件cpp，属性设置为 Project -\u0026gt; Property，在创建一份cpp文件后，属性设置变为 Project -\u0026gt; [project name] Property。不过，没什么大的问题，重新设置一下第5步提到的 directory 就可以了。\n","date":"2020-12-29T23:09:27+08:00","image":"https://yantree.github.io/p/visual-studio-opengl/topimg_hu2678ed699bd9406efb77dd8c989f9dae_380743_120x120_fill_box_smart1_2.png","permalink":"https://yantree.github.io/p/visual-studio-opengl/","title":"Visual Studio \u0026 OpenGL"},{"content":"技美——PBR 基础理论 PBR 全称 Physically Based Rendering，这是一个全新的，效果出众的光照模型，是近年来实时渲染领域的趋势。\nDiffusion \u0026amp; Reflection Diffusion 和 Reflection 也被叫做 “diffuse” “specular”，分别描述了物体表面和光的最基本的相互作用。绿色箭头部分的光是 specular，产生完美的反射现象，这种行为跟一个球碰撞到地面发生弹射的行为一样。红色箭头部分的光是 diffuse，会进入物体内部，这部分光要不被物体吸收（通常转化为热能），要不在物体内部散射，其中一部分会从物体表面散射出来而被重新看到。这种现象称为 “漫反射” 或更复杂一点的 “次表面散射”。\n 漫反射 \n吸收或散射根据物体表面颜色不同而不同（比如，如果物体表面呈现蓝色，表示的是物体表面吸收蓝色以外所有的光，散射出蓝色波长的光）。通常散射方向具有相当的随机性，我们可以认为散射的方向是任何方向。通常着色程序用一个颜色变量称为 “albedo” 或 “diffuse color” 来近似描述物体表面散射颜色。\n1. 不会进入物体部分的光经物体表面反射会形成高光区域 “specular”，依光源颜色决定，一般为白色。\n2. 进入物体部分的光会进一步在物体内部发生发射现象，出射方向是随机的，出射光 “diffuse color”，依物体属性决定\nTranslucency \u0026amp; Transparency 某些材质的漫反射要复杂一些——比如那些具有很长散射距离的材质：皮肤、蜡等的散射，通常一个简单的颜色变量是不够的，着色系统还需要考虑这些被照射物体的形状和厚度，如果物体足够薄，可以看到光从其背后散射出来，物体呈现半透明状；如果漫反射非常的小——比如玻璃，几乎没法注意到散射现象，光线完整的从物体的一边穿透到另一边，物体呈现全透明状。不同物体的次表面散射不尽相同，通常需要专门的“次表面散射”着色模型去模拟它。\n当物体漫反射非常的小，diffusion 会穿过物体，呈透明状\nEnergy Conservation 根据上面的描述我们得到一个结论：漫反射和镜面反射是互斥的。这是因为被物体散射的光线必须进入物体表面（那它就不能被镜面反射了）。这个结论符合“能量守恒”，也就是说离开表面的光不可能比原始的入射光要亮。着色系统很容易做到这一点：假设1表示100%光能，用1减去镜面反射的光，剩下的就属于漫反射部分。这意味着强烈高光的物体几乎没有漫反射现象，原因就是没有光进入到物体表面，大部分被镜面反射了。反之亦然。\n 能量守恒定律 \n能量守恒是PBR的一个重要概念。它可以保证美术合适的设置材质的反射率和albedo值，而不破坏物理规则。虽然在着色系统中强制能量守恒的物理限制并不等价于最后好看的着色效果，但起码可以使得渲染效果不至于背离物理规则太远，保证在不同光照条件下物体的光照表现一致性。\nMetals 金属作为最常见导电材质，有几点特性值得被特殊提及。 首先，金属大多比绝缘体更容易发生镜面反射。导体一般的镜面反射率高达60-90%，而绝缘体一般在0-20%的范围。这种高反射率阻止了大部分光到达其内部产生散射，使得金属看起来很闪亮。 其次，导体的反射率在可见光谱中呈现多样变化，使得它们的反射光具有颜色（白光照射下）。反射光具有颜色很奇怪，但确实在我们日常的材质中出现（比如，金、铜和黄铜）。绝缘体大部分情况下不会呈现出这种效果，它们的反射光的颜色是一般跟光源颜色一致。 最后，导体通常对进入其表面的光是吸收而不是散射。这意味着理论上导体不会表现出任何的漫反射，但实际中由于金属表面氧化等原因，还是会表现出部分散射效果。根据金属的这些特性呢，PBR着色系统用“metalness”作为输入来表示材料的金属程度，而不是albedo \u0026amp; reflectivity。\nMetals 属性，其反射光具有颜色\nFresnel Fresnel 现象是光照反射现象中不可或缺的部分。计算机图形学中 Fresnel 用来定义不同角度下的不同反射率——入射光方向越平行于物体表面，反射率越高。这意味着物体表面在 Fresnel 效果作用下，物体的边缘会更亮。大部分人可能已经对 Fresnel 效果已经有所了解，并且 Fresnel 效果在计算机图形中也不是新东西，然而，PBR 对 Fresnel 估算公式做了一些重要的纠正。 首先，入射光方向接近平行于物体表面时，一切光滑物体边缘表现为完美镜面反射，只要它足够光滑并且在合适的观察角度（也接近平行于物体表面）下，任何材质物体都表现为完美镜面反射。这有点违反直觉，但物理现象就是如此。 其次对 Fresnel 属性的观察发现不同材质的随入射光角度变化得到的 Fresnel 变化曲线和梯度差异并不大。对我们来讲意味着：如果我们期望渲染更加真实，美术对 Fresnel 行为的控制应该被降低，而不是被放大，或者说，没必要暴露多余的 Fresnel 参数让美术去调节。少了参数控制，就简化了美术内容生成，这是个利好。PBR 光照模型根据光泽度和反射率就可以自动去计算合适的 Fresnel 效果。\n 菲涅尔效应 \nFresnel 效果会随着物体表面的光滑度变低快速的变弱，接下来的内容会介绍到这些。\n入射光方向越平行于物体表面，反射率越高，越亮。\nMicrosurface 散射和反射都依赖物体表面的朝向。宏观上来看，物体表面朝向由物体的网格形状决定，或者是网格的法线贴图决定。渲染系统根据法线信息已经可以很好的渲染散射和反射。但是真实世界的表面在微观世界是不完美的：小坑，小裂缝和小块，这些不容易被肉眼看到的微观世界下的表面特性对散射和反射仍产生巨大影响。\n 模糊效果 \n上图中，平行的入射光线被粗糙的表面分散反射。因为光线发生碰撞的微表面的朝向各不相同，就像把球扔向凹凸不平的地面一样，球的弹射方向是不可预测的。简短的说，表面越粗糙，放射的光线越分散，呈现出“模糊”状。不过对每一个微表面进行反射估值在实时渲染计算中是不现实的，所以我们不直接描述微表面细节，而是通过一个粗糙度的概念和一个相当精确的光照模型得到接近的结果，这个通用的粗糙度叫做是 “Gloss”, “Smoothness” 或 “Roughness”。在材质中可以是一张贴图或一个固定值。材质中的微表面细节是非常重要的属性，它用来模拟真实世界中的各种微表面特征。光泽度贴图不是一个新概念，但是它在基于物理的着色中占有关键的地位，因为它对光的反射效果有决定性的影响。接下来我们将会看到。\n粗糙度，物体表面的凹凸程度\nEnergy Conservation (Again) 假设我们的着色系统已经考虑了微表面细节,反射多少入射光才是合适的是个值得研究的课题。光滑的表面会比粗糙的表面得到更加清楚的高光，这是符合能量守恒物理定律的：不同的材质反射了相同量级的入射光，但粗糙的表面反射的光线更加分散，看起来更模糊更暗，而光滑的表面反射更加集中，看起来更清晰更亮。\n 反射率 \nAll Hail Microsurface 基于上面的认识我们得到一个结论：物体微表面光泽度(粗糙度)直接影响了表面的光照表现。这意味着美术人员只用通过调整光泽贴图的形状和强度就可得到物体表面的划痕、凹痕、磨损或抛光等效果，而不额外需要高光遮罩贴图或反射率这些参数设置。\n微表面细节和反射率在物理上是相互联系的，就像之前描述的散射和反射一样，抛开它们之间的联系而单独分离的去设置它们有可能违背背后的光学物理规则。\n还有，对真实世界观察发现，材质之间的反射率的差异并不明显，比如水坑和泥巴，它们有非常相近的反射率，但泥巴非常粗糙，而水坑非常光滑，它们呈现出截然不同的反射表现。美术人员在创建这样的场景应该选择光泽度(粗糙度)而不是反射率来做为主要的材质差异设置项，见下图：\n 微表面细节 \n微表面属性对其他一些效果也有略微影响。比如，Fresnel效果在粗糙表面上会变弱，还有，大或凹微表面会“捕获”更多的光线，导致光在表面出线多次反射，从而被吸收的光量增加，亮度降低。不同的PBR系统处理这些细节的方式可能有些不同，最后呈现的结果也可能有些许不同，但总体还是遵守能量守恒的。\n总结  Reflection，反射光颜色为光源颜色。 Diffuse Color，散射光部分里，物体吸收除开 Diffuse Color 颜色之外的所有光，散射出 Diffuse Color 颜色的光。 Translucency \u0026amp; Transparency，考虑到物体的形状和厚度，如果物体足够薄，可以看到光从其背后散射出来，物体呈现半透明状；如果漫反射非常的小——比如玻璃，几乎没法注意到散射现象，光线完整的从物体的一边穿透到另一边，物体呈现全透明状。 Metals，反射光具有颜色，不受光源颜色左右，材料的金属程度。 Fresnel，入射光方向越平行于物体表面，反射率越高，越亮。 Microsurface，物体的光泽度。   原文连接：\n外文：https://marmoset.co/posts/basic-theory-of-physically-based-rendering/\n翻译：https://zhuanlan.zhihu.com/p/49564527\n ","date":"2020-11-27T09:59:25+08:00","image":"https://yantree.github.io/p/pbr%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA/topimg_hu85975aebbe83c38bfa94df6e1d3760eb_64270_120x120_fill_q75_box_smart1.jpg","permalink":"https://yantree.github.io/p/pbr%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA/","title":"PBR基础理论"},{"content":"Doom Emacs \u0026amp; Win10 Table of Contents\n Doom Emacs \u0026amp; Win10  准备工作 编辑系统环境变量 安装Doom All the icons    Doom Emacs，是一份成熟的 Emacs 配置文件，相较于自己一点一滴的积攒 Emacs 配置，Doom Emacs 能让你立即开始你的工作，把更多精力放在编码或者工作上而不是折腾配置文件。\n准备工作 以下方式是不通过第三方包管理器的安装配置过程，使用第三方包管理器会方便很多。\n 下载 Git 下载 Ripgrep 和 fd。确保你下载的是 Windows-gnu 版本。 下载主角 Emacs。  Git 是 .exe 文件，双击安装；解压缩 Ripgrep 和 fd，最好重命名一下文件夹分别叫 Ripgrep 和fd ，你可以把这两个文件夹放在你喜欢的位置。(后面会用到) Emacs 有 .exe 文件，可以直接安装，也有免安装版，不管使用什么方式，同上面 Ripgrep 和 fd 的使用一样，记住 Emacs 放在了哪儿。\n编辑系统环境变量   文件浏览器 -\u0026gt; 此电脑 -\u0026gt; 属性(左上角，第二个中间画 √ 的图标) -\u0026gt; 高级系统设置 -\u0026gt; 环境变量 -\u0026gt; 用户变量下面\n 新建，设置变量名为 HOME ，变量值为 C:\\Users\\USERNAME 点击 确认。 选中名为 Path 的变量，编辑 并添加 C:\\path\\to\\the\\emacs\\bin 文件路径，点击 确认。 选中名为 Path 的变量，编辑 并添加 C:\\path\\to\\the\\ripgrep 文件路径，点击 确认。 选中名为 Path 的变量，编辑 并添加 C:\\path\\to\\the\\fd 文件路径，点击 确认。 点击 确认。  所以要记住上面三个软件的安装目录，不要有遗漏的地方。\n  安装Doom  打开 git-bash.exe。 输入 cd ~，中间有个空格。 输入 git clone --depth 1 https://github.com/hlissner/doom-emacs ~/.emacs.d。 等下载完成后输入 cd .emacs.d/bin，执行 doom install，过程中，单击y 继续。 重复上面的操作 文件浏览器 -\u0026gt; 此电脑 -\u0026gt; 属性(左上角，第二个中间画 √ 的图标) -\u0026gt; 高级系统设置 -\u0026gt; 环境变量 -\u0026gt; 用户变量下面  选中名为 Path 的变量，编辑 并添加 C:\\Users\\USERNAME\\.emacs.d\\bin 文件路径，点击 确认。 点击 确认。    现在才安装完成，可以打开 Emacs 了。\nAll the icons 默认状态下，Windows 上没法自动安装字体，但 all the icons 这款插件有依赖于一些特殊的字体，所以我们要手动安装。\n 打开 Emacs ，M-x package-install RET all-the-icons RET 确保安装了 all the icons。 M-x all-the-icons-install-fonts ，完成后，找到字体下载的位置，双击安装。  现在你就可以使用 Doom Emacs。\n Happy Hacking! \n","date":"2020-11-06T14:17:09+08:00","image":"https://yantree.github.io/p/doom-emacs/topimg_hu1fc36ca756b092af5e34a851759d7448_71281_120x120_fill_box_smart1_2.png","permalink":"https://yantree.github.io/p/doom-emacs/","title":"Doom Emacs"},{"content":"Magit  A Git Porcelain inside Emacs。Magit\n Magit 可以看作是一个嵌在 Emacs 里的一个客户端，但它同时也提供了命令行的操作（废话）。在 Emacs 里打开一个 Git 项目，M-x: magit-status 这样就启动了 magit 了，之后下面提到的操作就非常简单了。\nTable of Contents\n Magit  1. Branch 2. Stage(all) / Unstage(all) 3. Commit 4. Log 5. Push 6. Reset 总结    1. Branch b\n magit-branch \n会弹出以上菜单，和你平时看到的完全不同（毕竟是在 Emacs）。\n其中紫色对应的是 按键 后面的白色字体就是具体功能的描述了。例如：再单击 c 我们就进入了创建新分支的命令。Branch\n2. Stage(all) / Unstage(all) 这里我们可以 Stage 单个文件，也可以 Stage 全部缓冲区的文件，相反，Unstage 也是这样的。\ns S 全选 , u U 全选 小写对应是单个缓冲区文件，大写对应的是全部缓冲区文件。Stage/Unstage\n3. Commit c\n magit-commit \n再单击一次 c 就会进入编辑 commit 的界面了，根据界面提示，C-c C-c 提交 commit ，C-c C-k 取消 commit 。Commit\n 上面 \u0026lsquo;-a\u0026rsquo;, \u0026lsquo;-e\u0026rsquo;\u0026hellip;的意思也是一样的，按 - a，不同的是带 - 前缀的功能只会在当前的操作上暂时启用，而且后面灰色的注释会暂时高亮，代表着暂时启用对应的。当你再一次按 c 时，后面的注释还是灰色，代表着没开启对应的功能。\n 4. Log l\n还是弹出一个与上面类似的菜单，这里就不展示图片了。再次单击，我们就能看到当前分支的所有 commit 记录。最常用的就是这个功能了，当然里面还有很多我还没使用过的功能。但是 在这个显示 LOG 记录的页面里，我们还可以轻松的完成 RESET， REVERT 这两个常用的操作。Log\n5. Push p\n在弹出菜单后，再按一次 p，就触发了 push 。Push\n magit-push-force-lease \n❗ 当我们执行回滚操作后，要将回滚后的仓库提交到远程仓库时，执行以上 push 的方法是行不通的，无法提交到远程仓库\n先按 p，进入 push 菜单；再按 - f，暂时开启 force with lease；然后再按 p 执行 push 操作，就能提交成功！\n 想要具体了解 force with lease 与 force 之间的区别，自己百度一下就明白了。\n 6. Reset X\n magit-reset \n上面的说明够直白清楚的了，在我们一般惯性思维里，会选择 X h，这种方式——一切都回到原来的那个状态。Reset\n 先按 l l 进入 Log，选择 Reset 的点之后，再进行上面的操作。\n 总结  直接在 Github 单击下载按钮下载的仓库是没有 .git 文件夹的，也就是说这样下载下来的不是一个仓库\u0026hellip;\u0026hellip;问题很大，所以还是使用 git clone ...... 的方式更稳妥，除非你不想要 .git 文件，也就是仓库提交记录。 在本地仓库新建 dev branch 时，先 p u，目的是在远程仓库创建也新建一个 dev branch，然后再 p p，不然会报错，你还不知道怎么弄。  ","date":"2020-06-21T09:14:22+08:00","image":"https://yantree.github.io/p/emacs-magit%E6%8F%92%E4%BB%B6/topimg_huffff616dccac0c778e7c8ccddfec01fc_19763_120x120_fill_box_smart1_2.png","permalink":"https://yantree.github.io/p/emacs-magit%E6%8F%92%E4%BB%B6/","title":"Emacs Magit✨插件"},{"content":"HUGO|Github Actions|Github Pages 使用方法主要参考了这篇 文章 。在这里我只是提炼一些关键点和遇到的坑。\n1. 绑定 SSH Key 如果是两个独立的仓库的，将 Public Key 添加到 \u0026lt;YourName\u0026gt;.github.io 仓库，打开 setting 面板，找到 Deploy keys 选项，Title： Deploy Github Page（看你个人喜好）。Key： Public Key ；将 Private Key 添加到 another repository 仓库，打开 setting 面板，找到 Secrets 选项，Name： ACTIONS_DEPLOY_KEY（后面会用到）。Value： Private Key 。这里有一个坑，要复制整个 Private Key 的值作为 Value ，不然后面会报错。这是具体的issues\n2. 配置 yml name:Github Pageson:push:branches:- masterjobs:deploy:runs-on:ubuntu-18.04steps:- uses:actions/checkout@v2#with:#submodules: true # Fetch Hugo themes (true OR recursive)#fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod- name:Setup Hugouses:peaceiris/actions-hugo@v2with:hugo-version:\u0026#39;0.71.1\u0026#39;# extended: true- name:Buildrun:hugo- name:Deployuses:peaceiris/actions-gh-pages@v3with:deploy_key:${{ secrets.ACTIONS_DEPLOY_KEY }}external_repository:YanTree/YanTree.github.iopublish_branch:master #推送的分支publish_dir:./public #要推送的文件夹commit_message:${{ github.event.head_commit.message }}注意调整上面 29-32 高亮行的值，ACTIONS_DEPLOY_KEY 对应的是上面 Secret 的 Name ，external_repository 改成你对应的仓库名 \u0026lt;YourName\u0026gt;/\u0026lt;YourName\u0026gt;.github.io\n反思 如果是使用 SSH 作为 Token ，在电脑重装系统或者更换电脑的情况下，我们每一次都要重新更新 Deploy keys ， Secrets 的值，对于我这种动不动就重装系统的人不太友好😁。所以我新建了一个 Personal access tokens 专门干这件事儿，目前看来是避免了上诉的痛点。本来是打算使用 GITHUB_TOKEN 的，都不要额外配置，但是这个 TOKEN 只适用于同一个仓库，这是具体的 issues，如果你是使用的同一个仓库的话，就可以使用这个功能了。\n","date":"2020-06-20T19:56:17+08:00","image":"https://yantree.github.io/p/github-actions-%E6%8E%A8%E9%80%81-github-page/topimg.svg","permalink":"https://yantree.github.io/p/github-actions-%E6%8E%A8%E9%80%81-github-page/","title":"Github Actions 推送 Github Page"},{"content":"Leaning Git and Github! 这是我自己学习使用 Git Github 时整理出的笔记，当时 Github 还没有出 官方教程，没有 论坛，现在都有了，这都是很好的学习资源，强烈推荐:+1:。这份文章列出了我学习时遇到的问题和解决方法，刚好趁这次疫情期间再整理一下，主要是将 ORG 文件转成 Markdown 文件的工作，因为 Github 对 org 文件格式并不友好，真的非常遗憾 :disappointed:！\nTable of Contents\n Leaning Git and Github!  config global user\u0026rsquo;s imformation git clone  1. 利用 Gitee 做工具人 2. 使用 cnpmjs镜像   解决git push代码到github上一直提示输入用户名及密码的问题  原因分析 解决办法   工作区的文件操作  撤销操作 添加和删除操作 移除版本控制   branch操作  创建操作 查看并切换 合并(merge) 删除分支(delete)   设置远程仓库 push an existing repository from the command line create a new repository on the command line 回滚 pull requests and merge  pull requests merge   总结    config global user\u0026rsquo;s imformation git config --global user.name \u0026#34;YourName\u0026#34; git config --global user.email \u0026#34;YourEmail@example.com\u0026#34; git clone 因为一些原因，国内克隆 GitHub 仓库的下载速度非常非常慢，这里提供两种解决办法。\n1. 利用 Gitee 做工具人 注册一个 Gitee 账号。点击右上角的 ➕ ，选择 从 Github/GitLab 导入仓库，导入完成后使用 Github 同样的方式克隆仓库就行了。git clone git@gitee.com:[YourName]/[YourRepository]  ，将 github 换成 gitee。这个方法有点繁琐，不过效果好。\n2. 使用 cnpmjs镜像 这个方法非常简单，只要修改地址后面的 github.com 为 github.com.cnpmjs.org\ngit clone https://github.com/[YourName]/[YourRepository] # 使用cnpmjs git clone https://github.com.cnpmjs.org/[YourName]/[YourRepository] 解决git push代码到github上一直提示输入用户名及密码的问题 原因分析 出现这种情况的原因是我们使用了 http 的方式 clone 代码到本地,相应的,也是使用 http 的方式将代码 push 到服务器.\n解决办法 解决办法很简单,将http方式改为ssh方式即可. 步骤：\n 先查看当前方式  git remote -v 把http方式改为ssh方式。先移除旧的http的origin  git remote rm origin 再添加新的ssh方式的origin  git remote add origin git@github.com:userName/repositoryName.git 改动完之后直接执行git push是无法推送代码的，需要设置一下上游要跟踪的分支,与此同时会自动执行一次git push命令,此时已经不用要求输入用户名及密码啦！​⚠​ 注意自己当前的分支，下面是推送到 master 分支。  git push --set-upstream origin master 工作区的文件操作 撤销操作 假设有文件 test01.txt和/test/test02.txt\n代码分别是：\ngit checkout -- test01.txt git checkout -- /test/test02.txt 添加和删除操作 添加\ngit add filename 删除\ngit rm filename 移除版本控制 git rm -r --cached file 在 .gitignore 中添加要移除的文件，最后：\ngit commit -m \u0026#39;modified .gitignore\u0026#39; branch操作 创建操作 我们创建dev分支，然后切换到dev分支\ngit checkout -b dev :exclamation: 解释上一条命令。\ngit checkout命令加上 -b 参数表示创建并切换，相当于以下两条命令：\ngit branch devgit checkout dev 查看并切换  查看 紧接着上面创建 dev 分支后，继续讲。带 * 号的就是当前分支。  git branch* dev master 切换至 master 分支  git checkout {master} 合并(merge) master ➡ dev\ngit merge dev 删除分支(delete) 删除 dev 分支\ngit branch -d devDeleted branch dev (was b17d20e). 设置远程仓库  创建SSH key  打开Shell，输入以下代码\nssh-keygen -t rsa -C \u0026#34;youremail@example.com\u0026#34; 之后不用管直接按两次 Enter 就创建完毕了。\n添加SSH密匙 创建完毕后用户目录下应该有个 .ssh 的文件夹。（:exclamation: 没有的话，可能是你没有显示隐藏文件）复制 id_rsa.pub 文件里的所有内容。  登录自己的 Github 账户，点击右上角头像图标。 点击 Settings 。 找到并点击新页面下的 SSH and GPG keys 。 右边的页面刷新后，点击 SSH keys 右边的 New SSH key 。 自己起一个 title，例如当前的设备 win10-work，在 Key 这里粘贴刚刚从 id_rsa.pub 文件里复制的所有内容。 最后点击 Add SSH key 大功告成。:tada:    push an existing repository from the command line git remote add origin git@github.com:userName/repositoryName.gitgit push -u origin master create a new repository on the command line echo \u0026#34;# .emacs.d(your repository name)\u0026#34; \u0026gt;\u0026gt; README.mdgit initgit add README.mdgit commit -m \u0026#34;first commit\u0026#34;git remote add origin git@github.com:userName/repositoryName.gitgit push -u origin master 回滚  查看当前版本库的状态  git log or\ngit log --pretty=oneline 回滚到上一次提交  git reset --hard HEAD^ or\ngit reset --hard [commit id(前五到六位数就够了)] ⚠ 注意\n 参数 hard 的作用 HEAD^^=HEAD~2 意思就是回滚到前两次提交的状态  如果想恢复到新版本  \u0026gt; git reflog 查看commit记录 根据commit id 进行回滚(见步骤2)    pull requests and merge pull requests 就是向一个分支提出合并请求，合并前弄清楚谁是被合并的分支\nmerge 在提出一个pull requests 请求后，我们可以合并分支，也可以不合并，这就是这一步要做的事情。在合并时也可能出现问题，到时候自行Google，目前的经历的一个问题，是通过删除或保留一些“问题”文件解决的。\n总结 除了一直提示输入用户名及密码的问题外，其他所有的问题都是比较容易解决的。当然我现在并没有使用官网命令行的 Git ，在接触了 Emacs 后，有一个叫 Magit 的 Emacs 杀手锏，类似于图形化界面的操作，我基本上就没用过命令行了。哈哈，having a nice day！\n","date":"2020-04-15T18:30:30+08:00","image":"https://yantree.github.io/p/git-%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%89%88%E6%9C%AC%E5%AD%A6%E4%B9%A0/topimg_hu8e2cda63a051da7ad17a0ebf4ac926f6_19259_120x120_fill_box_smart1_2.png","permalink":"https://yantree.github.io/p/git-%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%89%88%E6%9C%AC%E5%AD%A6%E4%B9%A0/","title":"Git 命令行版本学习"}]